{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 1: Sentiment Analysis with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test a **text classification** system for **sentiment analysis**, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using \n",
    "    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). \n",
    "    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** \n",
    "    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**1 mark**) **Tip: you should merge the two representations**\n",
    "\n",
    "\n",
    "\n",
    "- Binary Logistic Regression (LR) classifiers that will be able to accurately classify movie reviews trained with: \n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function (**1 mark**)\n",
    "    - Use L2 regularisation (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**2 marks). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**\n",
    "\n",
    "\n",
    "- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**1 mark**). \n",
    "\n",
    "\n",
    "- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!). If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**2 marks**). \n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible (you can find tips in Lab 1 sheet). Executing the whole notebook with your code should not take more than 5 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 14 Mar 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T13:41:17.642162Z",
     "start_time": "2020-03-27T13:41:16.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lable\n",
       "count  1400.000000\n",
       "mean      0.500000\n",
       "std       0.500179\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.500000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data_sentiment/train.csv', encoding= 'unicode_escape',names=['text','lable'])\n",
    "dev_data = pd.read_csv('data_sentiment/dev.csv', encoding= 'unicode_escape',names=['text','lable'])\n",
    "test_data = pd.read_csv('data_sentiment/test.csv', encoding= 'unicode_escape',names=['text','lable'])\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.900892Z",
     "start_time": "2020-02-15T14:17:28.891221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>you know that a movie has issues when most eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>while watching loser , it occurred to me that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>richard gere can be a commanding actor , but h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>though it is a fine piece of filmmaking , ther...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>if there is one thing that bothers me about ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  lable\n",
       "952  you know that a movie has issues when most eve...      0\n",
       "908  while watching loser , it occurred to me that ...      0\n",
       "221  richard gere can be a commanding actor , but h...      1\n",
       "310  though it is a fine piece of filmmaking , ther...      1\n",
       "218  if there is one thing that bothers me about ho...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_text = train_data['text'].tolist()\n",
    "Y_tr = train_data['lable'].to_numpy()\n",
    "\n",
    "dev_data_text = dev_data['text'].tolist()\n",
    "Y_dev = dev_data['lable'].to_numpy()\n",
    "\n",
    "test_data_text = test_data['text'].tolist()\n",
    "Y_te = test_data['lable'].to_numpy()\n",
    "\n",
    "len(dev_data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Text \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its',\n",
    "              '4','8','t','d','s','b','ve','','dr','l','x','your','mother','woman'] \n",
    "\n",
    "# add some high frequent but meaningless word like the '4','8','t','d','s','b','ve','dr','x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `char_ngrams`: boolean. If true the function extracts character n-grams\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x': a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "                   stop_words=[], vocab=set(), char_ngrams=False):\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    split_text = re.split(r'\\W+',x_raw)                      # separate docs to word to drop the stop_words\n",
    "    \n",
    "    for word in split_text:\n",
    "        if word in stop_words:\n",
    "            split_text.remove(word)\n",
    "\n",
    "    if(char_ngrams == False):                                # if it's BOW \n",
    "        if len(vocab)>0:                                     # if give the vocab, just extract the ngrams in vocab\n",
    "            x = [''.join(split_text[i:i+j]) for i in range(len(split_text)) \n",
    "                    for j in range(ngram_range[0],ngram_range[1]+1) \n",
    "                 if(i+j<=len(split_text) and ''.join(split_text[i:i+j]) in vocab)]\n",
    "        \n",
    "            \n",
    "        else:                                                # when do not give a vocab, save all ngrams\n",
    "            \n",
    "            x = [' '.join(split_text[i:i+j]) for i in range(len(split_text)) \n",
    "                          for j in range(ngram_range[0],ngram_range[1]+1) \n",
    "                 if(i+j<=len(split_text) and ''.join(split_text[i:i+j]) not in stop_words)]       \n",
    "\n",
    "    else:                                                    # if it's BOCN\n",
    "        text = ''.join(split_text)                           # jiont the word to a list\n",
    "        split_text = re.split(r'\\W*', text)                  # separate it to characters\n",
    "        split_text.remove(split_text[0])                     # remove the signal of string\n",
    "        split_text.remove(split_text[-1])\n",
    "        \n",
    "        if len(vocab)>0:                                     # when give a vocab, just extract the ngrams in vocab\n",
    "    \n",
    "            x = [''.join(split_text[i:i+j]) for i in range(len(split_text)) \n",
    "                    for j in range(ngram_range[0],ngram_range[1]+1) \n",
    "                 if(i+j<=len(split_text) and ''.join(split_text[i:i+j]) in vocab)]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            x = [''.join(split_text[i:i+j]) for i in range(len(split_text)) \n",
    "                    for j in range(ngram_range[0],ngram_range[1]+1) \n",
    "                 if(i+j<=len(split_text) and ''.join(split_text[i:i+j]) not in stop_words)]\n",
    "        \n",
    "                            \n",
    "                        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting character n-grams the function should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mo',\n",
       " 'mov',\n",
       " 'movi',\n",
       " 'ov',\n",
       " 'ovi',\n",
       " 'ovie',\n",
       " 'vi',\n",
       " 'vie',\n",
       " 'vieg',\n",
       " 'ie',\n",
       " 'ieg',\n",
       " 'iego',\n",
       " 'eg',\n",
       " 'ego',\n",
       " 'egoo',\n",
       " 'go',\n",
       " 'goo',\n",
       " 'good',\n",
       " 'oo',\n",
       " 'ood',\n",
       " 'od']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"movie good\", \n",
    "               ngram_range=(2,4), \n",
    "               stop_words=[],\n",
    "               char_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary \n",
    "\n",
    "The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_vocab function to get the train dataset vocab\n",
    "\n",
    "def get_vocab(X_raw, ngram_range=(1,3), \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[],char_ngrams=False):\n",
    "    \n",
    "    df = []\n",
    "    vocab = []\n",
    "    df_save =[]\n",
    "    ngram_counts = []                                   # do not use in after procedure, so not calculate \n",
    "    ngram_counts_temp = []                              # for saving runtime\n",
    "    x_ngram = [[] for i in range(len(X_raw))]\n",
    "    \n",
    "    for text in X_raw:                                  # extract the ngram from each doc \n",
    "        docs = extract_ngrams(text,ngram_range,stop_words =stop_words,char_ngrams=char_ngrams)\n",
    "        ngram_counts_temp.append(docs)\n",
    "        df_save.append(set(docs)) \n",
    "\n",
    "    df_save = [i for k in df_save for i in k]\n",
    "    temp = Counter(df_save)\n",
    "    if keep_topN == 0:                                  # if drop less minimum frequent ngram \n",
    "        temp.pop(ngram for ngram,num in dict(temp).items() if num < min_df)\n",
    "        \n",
    "    else:                                               # if extract topN frequent ngram\n",
    "        sort_temp = sorted(dict(temp).items(), key=lambda item: item[1], reverse=True)\n",
    "        temp = sort_temp[:keep_topN]\n",
    "            \n",
    "                \n",
    "    df=dict(temp)\n",
    "    vocab=list(df.keys())\n",
    "    \n",
    "    for i in range(len(X_raw)):                        # calculate the training dataset ngram list\n",
    "        x_ngram[i] = [word for word in ngram_counts_temp[i] if word in vocab]\n",
    "    \n",
    "    return vocab, df, ngram_counts,x_ngram             # increase the train ngram list for saving the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_range=(1,3)\n",
    "min_df = 50\n",
    "#keep_topN = 0                                         # if using min_df\n",
    "keep_topN = 2000\n",
    "char_ngrams=False\n",
    "\n",
    "vocab,df,ngram_counts,ngram_train = get_vocab(train_data_text, ngram_range, min_df,\n",
    "                                              keep_topN,stop_words=stop_words,char_ngrams=char_ngrams)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "def createDict(vocab):\n",
    "    \n",
    "    id2word = dict()\n",
    "    word2id = dict()\n",
    "    \n",
    "    for i,voc in enumerate(vocab):\n",
    "        id2word[i] = voc\n",
    "        word2id[voc] = i\n",
    "        \n",
    "    return id2word,word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word,word2id = createDict(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.213253Z",
     "start_time": "2020-02-15T14:17:39.329147Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the features in development dataset and test dataset, the features in training dataset has already save \n",
    "# when call the get_vocab for saving the runtime.\n",
    "\n",
    "def ext_all_ngrams(dev,test,stop_words,vocab,char_ngrams):   \n",
    "    \n",
    "    ngram_dev=[]\n",
    "    ngram_test=[]\n",
    "    \n",
    "    for text in dev:\n",
    "        docs = extract_ngrams(text,ngram_range,stop_words = stop_words,vocab = vocab,char_ngrams=char_ngrams)\n",
    "        ngram_dev.append(docs)\n",
    "        \n",
    "    for text in test:\n",
    "        docs = extract_ngrams(text,ngram_range,stop_words = stop_words,vocab = vocab,char_ngrams=char_ngrams)\n",
    "        ngram_test.append(docs)\n",
    "        \n",
    "    return ngram_dev,ngram_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_dev,ngram_test = ext_all_ngrams(dev_data_text,test_data_text,stop_words,vocab,char_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    \n",
    "    rows, cols = len(X_ngram),len(vocab)                   \n",
    "    X_vec = [([]*cols) for i in range(rows)]         # create an empty matrix to save the vectors\n",
    "    \n",
    "    for i,doc in enumerate(X_ngram):\n",
    "        doc_fre = Counter(doc)                       # the word frequency in a documents\n",
    "        for j,word in enumerate(vocab):              # save the word frequency according to the order of vocab\n",
    "            if doc_fre[word]>0:\n",
    "                X_vec[i].append(doc_fre[word])\n",
    "            else:\n",
    "                X_vec[i].append(0)\n",
    "                \n",
    "    X_vec = np.squeeze(np.asarray(X_vec))            # matrix transfer to array\n",
    "    \n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [],
   "source": [
    "def vec_all_data(ngram_train,ngram_dev,ngram_test,vocab):       # make a counts matrix of features with documents\n",
    "    X_tr = vectorise(ngram_train,vocab)\n",
    "    X_dev = vectorise(ngram_dev,vocab)\n",
    "    X_te = vectorise(ngram_test,vocab)\n",
    "    return X_tr,X_dev,X_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_count_w,x_dev_count_w,x_te_count_w = vec_all_data(ngram_train,ngram_dev,ngram_test,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_tfidf(X_tr_count,X_dev_count,X_te_count,vocab,df):\n",
    "    \n",
    "    idf_dict = dict()                                 # to save the idf of word\n",
    "\n",
    "    for word in vocab:                                # calculate the idf\n",
    "        idf_dict[word] = np.log10(np.shape(X_tr_count)[0]/df[word])\n",
    "    \n",
    "    idf_list = list(idf_dict.values())\n",
    "    idf_arr = np.array(idf_list)\n",
    "\n",
    "    X_tr_tfidf = X_tr_count*idf_arr                   # tfidf = tf * idf\n",
    "    X_dev_tfidf = X_dev_count*idf_arr\n",
    "    X_te_tfidf = X_te_count*idf_arr\n",
    "    \n",
    "    return X_tr_tfidf,X_dev_tfidf,X_te_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tfidf,X_dev_tfidf,X_te_tfidf = vec_tfidf(x_tr_count_w,x_dev_count_w,x_te_count_w,vocab,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    preds_proba = sigmoid(weights@X.T) \n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    y = weights@(X.T)               # predict the probability\n",
    "    \n",
    "    y_class = []\n",
    "    \n",
    "    for p in y:\n",
    "        if(p>0.5):\n",
    "            y_class.append(1)\n",
    "        else:\n",
    "            y_class.append(0)\n",
    "            \n",
    "    return np.array(y_class)        # return the most probable class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, Y_pre, weights, alpha=0.00001):\n",
    "    '''\n",
    "    Binary Cross-entropy Loss\n",
    "\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    l = (-Y*np.log(Y_pre)-(1-Y)*np.log(1-Y_pre)).mean()+alpha*np.dot(weights.T,weights)\n",
    "\n",
    "    return l\n",
    "\n",
    "# attention: derivation of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.000001,             # trianing model using SGD\n",
    "        alpha=0.000001, epochs=300, \n",
    "        tolerance = 0.0000001, print_progress=True):\n",
    "    \n",
    "    last_val_loss = 1e+3                   \n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    weights = np.zeros(np.shape(X_tr)[1])\n",
    "    Y_tr_pre = np.zeros(np.shape(X_tr)[0])\n",
    "    for i in range(epochs):                                      \n",
    "        indices = np.array(range(len(X_tr)))                     # random the training dataset each epochs\n",
    "        np.random.shuffle(indices)\n",
    "        for index in indices:\n",
    "            Y_tr_pre[index] = predict_proba(X_tr[index],weights)                             # predicting \n",
    "            weights -= lr*((Y_tr_pre[index] - Y_tr[index])*X_tr[index] + 2*alpha*weights)    # updating the w\n",
    "        \n",
    "        training_loss_history.append([i,binary_loss(X_tr,Y_tr,Y_tr_pre,weights,alpha)])      # saving the train loss\n",
    "        Y_dev_pre = predict_proba(X_dev,weights)\n",
    "        validation_loss_history.append([i,binary_loss(X_dev,Y_dev,Y_dev_pre,weights,alpha)]) # saving the val loss\n",
    "        if print_progress == True:                               # when need to print the process\n",
    "            if (i+1)%200 ==0:                                    # show the process every 200 epochs \n",
    "                print(\"Epoch: {}. Training_Loss: {:.6f}. Val_Loss: {:.6f}\".       # cause there are too many epochs\n",
    "                      format((i+1),training_loss_history[i][1],validation_loss_history[i][1]))\n",
    "        if (last_val_loss-validation_loss_history[i][1]) < tolerance:             # if the difference between new and \n",
    "            break                                                                 # last val loss, break the loop\n",
    "        last_val_loss = validation_loss_history[i][1]            \n",
    "    \n",
    "    print('Finish training! Epoch: {}. Training_Loss: {:.6f}. Val_Loss: {:.6f}. lr:{:.8f}'.\n",
    "          format((i+1),training_loss_history[i][1],validation_loss_history[i][1],lr))\n",
    "    \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200. Training_Loss: 0.485190. Val_Loss: 0.543535\n",
      "Epoch: 400. Training_Loss: 0.412434. Val_Loss: 0.495386\n",
      "Epoch: 600. Training_Loss: 0.368043. Val_Loss: 0.467932\n",
      "Epoch: 800. Training_Loss: 0.336199. Val_Loss: 0.449687\n",
      "Epoch: 1000. Training_Loss: 0.311513. Val_Loss: 0.436656\n",
      "Epoch: 1200. Training_Loss: 0.291472. Val_Loss: 0.426931\n",
      "Epoch: 1400. Training_Loss: 0.274688. Val_Loss: 0.419451\n",
      "Epoch: 1600. Training_Loss: 0.260311. Val_Loss: 0.413550\n",
      "Epoch: 1800. Training_Loss: 0.247785. Val_Loss: 0.408877\n",
      "Epoch: 2000. Training_Loss: 0.236724. Val_Loss: 0.405083\n",
      "Epoch: 2200. Training_Loss: 0.226851. Val_Loss: 0.401985\n",
      "Epoch: 2400. Training_Loss: 0.217958. Val_Loss: 0.399463\n",
      "Epoch: 2600. Training_Loss: 0.209888. Val_Loss: 0.397401\n",
      "Epoch: 2800. Training_Loss: 0.202516. Val_Loss: 0.395702\n",
      "Epoch: 3000. Training_Loss: 0.195746. Val_Loss: 0.394321\n",
      "Finish training! Epoch: 3095. Training_Loss: 0.192716. Val_Loss: 0.393758. lr:0.00000360\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000036    \n",
    "alpha = 0.0001\n",
    "epochs = 3400\n",
    "tolerance = 0.000001\n",
    "\n",
    "w_count, train_loss, val_loss = SGD(x_tr_count_w,Y_tr,x_dev_count_w,Y_dev,lr,alpha,epochs,tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss(train_loss,val_loss,t):\n",
    "    training_loss_history = np.array(train_loss)\n",
    "    validation_loss_history = np.array(val_loss)\n",
    "\n",
    "    x1 = training_loss_history[:,0]\n",
    "    y1 = training_loss_history[:,1]\n",
    "    x2 = validation_loss_history[:,0]\n",
    "    y2 = validation_loss_history[:,1]\n",
    "\n",
    "    fig = plt.figure(figsize = (7,5))       \n",
    "\n",
    "    p1 = plt.plot(x1, y1,'g-', label = u'Train Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    p2 = plt.plot(x2, y2, 'b-', label = u'Val Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel(u'Epochs')\n",
    "    plt.ylabel(u'Loss')\n",
    "    plt.title('Compare loss for {}'.format(t))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 'BOCN-Count'\n",
    "show_loss(train_loss,val_loss,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's right. Because as the picture shows, both loss decrease slowly and gradually. There no wave and the difference between the new epoch loss is less than the tolenrance finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_te,Y_te,w_count):\n",
    "    preds_te = predict_class(X_te, w_count)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "    print('Precision:', precision_score(Y_te,preds_te))\n",
    "    print('Recall:', recall_score(Y_te,preds_te))\n",
    "    print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Precision: 0.8837209302325582\n",
      "Recall: 0.76\n",
      "F1-Score: 0.8172043010752689\n"
     ]
    }
   ],
   "source": [
    "evaluation(x_te_count_w,Y_te,w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.613935Z",
     "start_time": "2020-02-15T14:17:51.610660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "unfortunately\n",
      "worst\n",
      "boring\n",
      "script\n",
      "why\n",
      "nothing\n",
      "only\n",
      "plot\n",
      "any\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_count.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.624122Z",
     "start_time": "2020-02-15T14:17:51.615674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "well\n",
      "fun\n",
      "seen\n",
      "both\n",
      "movies\n",
      "world\n",
      "hilarious\n",
      "also\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_count.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top 10 word of each class\n",
    "\n",
    "def print_top10(w,id2word):                          \n",
    "    neg_temp = []\n",
    "    pos_temp = []\n",
    "    top_neg = w.argsort()[:10]\n",
    "    for i in top_neg:\n",
    "        neg_temp.append(id2word[i])\n",
    "    top_pos = w.argsort()[::-1][:10]\n",
    "    for i in top_pos:\n",
    "        pos_temp.append(id2word[i])\n",
    "    print('Top 10 negative word:{}'.format(neg_temp))\n",
    "    print('Top 10 positive word:{}'.format(pos_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In my opinian, the some of negtive word can use in other field like bad, worst,unfortunately,nothing,boring and so on. And some of positive word such as great, well can also use in a different domain but the some of these positive word are neutral like life, movies,both,world. The word like seen movies may not be applied in other field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| LR | lr  | alpha | ngram_range  |  keep_topN  | training epochs | accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 2e-6  | 0.0001  |(1,3) |  2500 | 3813  |  0.8275  |\n",
    "|            | 2.3e-6  | 0.0001 |(1,3)  | 2500  | 3776  |  0.83  |\n",
    "|            | 2.6e-6  | 0.0001 |(1,3)  | 2500  | 2823  |  0.8275  |\n",
    "|            | 2.3e-6  | 0.0001 | (1,3) | 3000  | 3114   |  0.835  |\n",
    "|            | 2.3e-6  | 0.000001 | (1,3) | 3000 | 3013 | 0.8325|\n",
    "|            | 2.3e-6  | 0.001 | (1,3) | 3000 | 2946 | 0.8225|\n",
    "| BOW-tfidf  |  1e-5 | 0.0001 |(1,3)  | 3000  |  3258  | 0.84 |\n",
    "|            |  1e-5 | 0.0001 | (1,3)  | 2000  | 2523 | 0.8625|\n",
    "| BOCN-count  |  3e-7 | 0.000001 | (3,4)  |  500   |   1140  |  0.7375  |\n",
    "|             | 1.7e-7| 0.000001| (3,4) | 1000 | 2451 | 0.7875|\n",
    "| BOCN-tfidf  |  3e-5 |0.0001 | (3,4) |  500 |  1186  |   0.7  |\n",
    "|             |   3e-5 | 0.0001| (3,4) |  700 |  1483  |   0.75  |\n",
    "|             | 3e-5 | 0.0001|(3,4)| 1000| 596 | 0.775 |\n",
    "|             | 3e-5 | 0.001|(3,4)| 1000| 590 | 0.7725 |\n",
    "|             | 3e-5 | 0.00001|(3,4)| 1000| 636 | 0.7725 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1. When choose the model hyperparameters like learning rate and regularisation strength, I always chooce a common value like 1e-5 or 1e-6, looking at the training epochs. Because we have build a threshold, so when learning rate is too high, it will break fast and the accuracy is not ideal. And when the learning rate is too small, the epochs may become too much and when we set a maximum value of epochs like 3000, it will not break untill 3000 epochs and the accuracy is also not good enough.\n",
    "2. Training epochs will become large when learning rate become smaller.\n",
    "3. According to the equation of the w updating, it influences the speed of w updating, when it become smaller, the more epochs needed because the speed of w updating become slower. So when we tune the regularisation strength smaller, we can tune the learning rate larger to remain the model's performance.The effect of regularisation strength is quite small but when it is 1e-4, the accuracy become higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning for each model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packaging the LR model with SGD for reusing\n",
    "\n",
    "def myLRmodel_count(X_tr, Y_tr, X_dev,Y_dev,X_te,Y_te, ngram_range,min_df,keep_topN,\n",
    "                    stop_words,char_ngrams,lr,alpha,print_progress,t,epochs,tolerance):\n",
    "    \n",
    "    vocab,df,ngram_counts,ngram_tr = get_vocab(X_tr, ngram_range=ngram_range, min_df=min_df,\n",
    "                                      keep_topN=keep_topN,stop_words=stop_words,\n",
    "                                               char_ngrams=char_ngrams)             # get vocab\n",
    "    \n",
    "    ngram_dev,ngram_te = ext_all_ngrams(X_dev,X_te,stop_words,vocab,char_ngrams)    # extract the ngrams \n",
    "    \n",
    "    x_tr_c,x_dev_c,x_te_c = vec_all_data(ngram_tr,ngram_dev,ngram_te,vocab)         # vectorise the ngrams\n",
    "    \n",
    "    id2word,word2id = createDict(vocab)\n",
    "    \n",
    "    if isinstance(lr,list):                       # determine if lr is a list, that is, whether tune\n",
    "        \n",
    "        max_score = 0\n",
    "        \n",
    "        for l in lr:\n",
    "            weights_temp,train_loss_temp,val_loss_temp = SGD(x_tr_c,Y_tr,x_dev_c,Y_dev, lr=l, alpha=alpha, \n",
    "                                      epochs=epochs,tolerance = tolerance, print_progress=print_progress)\n",
    "                                                  # train the model\n",
    "                \n",
    "            preds_te = predict_class(x_te_c, weights_temp)\n",
    "            score = accuracy_score(Y_te,preds_te)\n",
    "            \n",
    "            if score > max_score:                 # save the best para and information related to save runtime \n",
    "                best_lr = l                       # or it will train the model again using the best parameter\n",
    "                max_score = score\n",
    "                weights = weights_temp\n",
    "                train_loss = train_loss_temp\n",
    "                val_loss = val_loss_temp\n",
    "                \n",
    "        print('Train finish! The best parameter:{:.8f}, accuracy:{} '.format(best_lr,max_score))\n",
    "                \n",
    "    else:                                          # if lr is not a list, do not need tune\n",
    "        weights,train_loss,val_loss = SGD(x_tr_c,Y_tr,x_dev_c,Y_dev, lr=lr, alpha=alpha, \n",
    "                                      epochs=epochs,tolerance = tolerance, print_progress=print_progress)\n",
    "                                                  # train the model\n",
    "        print('Train finish!')\n",
    "\n",
    "    show_loss(train_loss,val_loss,t)\n",
    "    \n",
    "    evaluation(x_te_c,Y_te,weights)\n",
    "    \n",
    "    print_top10(weights,id2word)\n",
    "    \n",
    "    return x_tr_c,x_dev_c,x_te_c,vocab,df,id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf LR model, evaluate results and show the loss\n",
    "\n",
    "def myLRmodel_tfidf(x_tr_count,Y_tr,x_dev_count,Y_dev,x_te_count,Y_te, vocab,df,ngram_range,     \n",
    "                    char_ngrams,lr,alpha,epochs,tolerance,print_progress,t,id2word):\n",
    "    \n",
    "    x_tr_tfidf,x_dev_tfidf,x_te_tfidf = vec_tfidf(x_tr_count,x_dev_count,x_te_count,vocab,df)\n",
    "    \n",
    "    \n",
    "    if isinstance(lr,list):                       # determine if lr is a list, that is, whether tune\n",
    "        \n",
    "        max_score = 0\n",
    "        for l in lr:\n",
    "            weights_temp,train_loss_temp,val_loss_temp = SGD(x_tr_tfidf,Y_tr,x_dev_tfidf,Y_dev, lr=l, alpha=alpha, \n",
    "                                          epochs=epochs,tolerance = tolerance, print_progress=print_progress)\n",
    "                                                  # train the model\n",
    "            preds_te = predict_class(x_te_tfidf, weights_temp)\n",
    "            score = accuracy_score(Y_te,preds_te)\n",
    "            \n",
    "            if score > max_score:                 # save the best para and information related to save runtime \n",
    "                best_lr = l                       # or it will train the model again using the best parameter\n",
    "                max_score = score\n",
    "                weights = weights_temp\n",
    "                train_loss = train_loss_temp\n",
    "                val_loss = val_loss_temp\n",
    "                \n",
    "        print('Train finish! The best parameter:{:.8f}, accuracy:{} '.format(best_lr,max_score))\n",
    "                \n",
    "    else:                                          # if lr is not a list, do not need tune\n",
    "        weights,train_loss,val_loss = SGD(x_tr_tfidf,Y_tr,x_dev_tfidf,Y_dev, lr=lr, alpha=alpha, \n",
    "                                          epochs=epochs,tolerance = tolerance, print_progress=print_progress)\n",
    "                                                  # train the model\n",
    "        print('Train finish!')\n",
    "        \n",
    "    show_loss(train_loss,val_loss,t)              # show the loss line\n",
    "    \n",
    "    evaluation(x_te_tfidf,Y_te,weights)           # evaluate the model\n",
    "    \n",
    "    print_top10(weights,id2word)\n",
    "    \n",
    "    return x_tr_tfidf,x_dev_tfidf,x_te_tfidf      # using for BOW+BOCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training! Epoch: 2358. Training_Loss: 0.098958. Val_Loss: 0.376396. lr:0.00001500\n",
      "Train finish!\n",
      "Accuracy: 0.855\n",
      "Precision: 0.8622448979591837\n",
      "Recall: 0.845\n",
      "F1-Score: 0.8535353535353536\n",
      "Top 10 negative word:['bad', 'worst', 'boring', 'unfortunately', 'supposed', 'awful', 'nothing', 'minute', 'waste', 'ridiculous']\n",
      "Top 10 positive word:['hilarious', 'great', 'pulp', 'fun', 'perfectly', 'terrific', 'simple', 'truth', 'definitely', 'overall']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1.5e-5                                 # if no need to tune\n",
    "#lr = list(np.logspace(-5.1, -4.9, 3))      # tuning the lr\n",
    "t='BOW-tfidf'\n",
    "print_progress = False\n",
    "x_tr_tfidf_w,x_dev_tfidf_w,x_te_tfidf_w = myLRmodel_tfidf(x_tr_count_w,Y_tr,x_dev_count_w,Y_dev,x_te_count_w,Y_te,vocab,\n",
    "                df,ngram_range,char_ngrams,lr,alpha,epochs,tolerance,print_progress,t,id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word in the top 10 in BOW-tfidf model have more generate features. Such as in negative word, unless 'minute', other words can definitely apply in every domain. There are some positive word can be alse applied as 'hilarious','great','fun','perfectly'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/m7r6fd_d4730hzvw8f6cqb0m0000gn/T/ipykernel_90992/757702127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m x_tr_count_c,x_dev_count_c,x_te_count_c,vocab_c,df_c,id2word_c = myLRmodel_count(train_data_text, Y_tr, dev_data_text,Y_dev,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                              \u001b[0mtest_data_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_topN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                 \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar_ngrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/m7r6fd_d4730hzvw8f6cqb0m0000gn/T/ipykernel_90992/2122801858.py\u001b[0m in \u001b[0;36mmyLRmodel_count\u001b[0;34m(X_tr, Y_tr, X_dev, Y_dev, X_te, Y_te, ngram_range, min_df, keep_topN, stop_words, char_ngrams, lr, alpha, print_progress, t, epochs, tolerance)\u001b[0m\n\u001b[1;32m      4\u001b[0m                     stop_words,char_ngrams,lr,alpha,print_progress,t,epochs,tolerance):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     vocab,df,ngram_counts,ngram_tr = get_vocab(X_tr, ngram_range=ngram_range, min_df=min_df,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                       \u001b[0mkeep_topN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_topN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                char_ngrams=char_ngrams)             # get vocab\n",
      "\u001b[0;32m/var/folders/v3/m7r6fd_d4730hzvw8f6cqb0m0000gn/T/ipykernel_90992/872502656.py\u001b[0m in \u001b[0;36mget_vocab\u001b[0;34m(X_raw, ngram_range, min_df, keep_topN, stop_words, char_ngrams)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                        \u001b[0;31m# calculate the training dataset ngram list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram_counts_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_ngram\u001b[0m             \u001b[0;31m# increase the train ngram list for saving the runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/m7r6fd_d4730hzvw8f6cqb0m0000gn/T/ipykernel_90992/872502656.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                        \u001b[0;31m# calculate the training dataset ngram list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram_counts_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_ngram\u001b[0m             \u001b[0;31m# increase the train ngram list for saving the runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BOCN-count\n",
    "\n",
    "t = 'BOCN-count'\n",
    "lr = 1.5e-7               # if don't need to tune the lr\n",
    "#lr = list(np.logspace(-7,-6.8,4))\n",
    "alpha = 0.001\n",
    "min_df = 700\n",
    "epochs = 3000\n",
    "#keep_topN = 0                 # if only drop the df of word less than 700\n",
    "keep_topN = 2000\n",
    "tolerance = 0.000001\n",
    "ngram_range = (3,4)\n",
    "char_ngrams = True\n",
    "print_progress = True\n",
    "\n",
    "x_tr_count_c,x_dev_count_c,x_te_count_c,vocab_c,df_c,id2word_c = myLRmodel_count(train_data_text, Y_tr, dev_data_text,Y_dev,\n",
    "                                                             test_data_text,Y_te,ngram_range,min_df,keep_topN,\n",
    "                                                                stop_words,char_ngrams,lr,alpha,\n",
    "                                                             print_progress,t,epochs,tolerance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOCN-tfidf\n",
    "\n",
    "t='BOCN-tfidf'\n",
    "lr = 4e-6\n",
    "#lr = list(np.logspace(-4.9,-5.1,3))\n",
    "alpha = 0.001\n",
    "epochs = 4000\n",
    "print_progress = False\n",
    "x_tr_tfidf_c,x_dev_tfidf_c,x_te_tfidf_c = myLRmodel_tfidf(x_tr_count_c,Y_tr,x_dev_count_c,Y_dev,x_te_count_c,Y_te,vocab_c,\n",
    "                df_c,ngram_range,char_ngrams,lr,alpha,epochs,tolerance,print_progress,t,id2word_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW+BOCN\n",
    "t = 'BOW+BOCN'\n",
    "lr = 7.4e-6\n",
    "epochs = 3000\n",
    "alpha = 0.001\n",
    "print_progress = True\n",
    "\n",
    "x_tr_bb = np.hstack([x_tr_tfidf_w,x_tr_tfidf_c])\n",
    "x_dev_bb = np.hstack([x_dev_tfidf_w,x_dev_tfidf_c])\n",
    "x_te_bb = np.hstack([x_te_tfidf_w,x_te_tfidf_c])\n",
    "vocab_bb = vocab + vocab_c\n",
    "id2wrod_bb,word2id_bb = createDict(vocab_bb)\n",
    "\n",
    "weights,train_loss,val_loss = SGD(x_tr_bb,Y_tr,x_dev_bb,Y_dev, lr=lr, alpha=alpha, \n",
    "                                      epochs=epochs,tolerance = tolerance, print_progress=print_progress)\n",
    "    \n",
    "evaluation(x_te_bb,Y_te,weights)\n",
    "    \n",
    "show_loss(train_loss,val_loss,t) \n",
    "\n",
    "print_top10(weights,id2wrod_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.883 |  0.760 | 0.817  |\n",
    "| BOW-tfidf  | 0.862  | 0.845  | 0.853  |\n",
    "| BOCN-count  | 0.909  | 0.700  | 0.790  |\n",
    "| BOCN-tfidf  | 0.907  |  0.685 |  0.780 |\n",
    "| BOW+BOCN  | 0.909  | 0.800  | 0.851  |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "As the table shows, the BOCN model perform not as good as BOW from F1-score, the reason may that the characters may has less meaning than the word or the learning rate and regulation do not fit mostly or it may need more features to train the model. The best model may be the BOW-tdidf or BOW+BOCN model. Because these the F1-score of these two model two model have quite a little difference and it may be changed by the hyperparameters or the size of features. And the reason of these two model having a good perform may be firstly, they are tfidf frequency. What's more ,according to BOW+BOCN, it has the largest features, on the theory of model, it may perform better. And these two model perform better than other may because they have a higher recall, which relate the threshold. When the threshold become higher, the prediction of positive sample may become more conservative, which contribute to a lower recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
