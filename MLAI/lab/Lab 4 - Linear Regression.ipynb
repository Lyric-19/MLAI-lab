{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Linear Regression\n",
    "\n",
    "### Modified by Mauricio √Ålvarez, 13th October 2021\n",
    "\n",
    "### 13th October 2015 Neil Lawrence\n",
    "\n",
    "\n",
    "## Sum of Squares Error\n",
    "\n",
    "Minimizing the sum of squares error was first proposed by [Legendre](http://en.wikipedia.org/wiki/Adrien-Marie_Legendre) in 1805. His book, which was on the orbit of comets, is available on google books, we can take a look at the relevant page by calling the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"500\"\n",
       "            src=\"http://books.google.co.uk/books?id=spcAAAAAMAAJ&pg=PA72&output=embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1135f5358>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'http://books.google.co.uk/books?id=spcAAAAAMAAJ&pg=PA72&output=embed'\n",
    "width=700 \n",
    "height=500\n",
    "from IPython.display import IFrame\n",
    "IFrame(target, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous cell, you should see the Google books link embedded in the notebook. If you can't display it, got directly to the Google books link [here](http://books.google.co.uk/books?id=spcAAAAAMAAJ&pg=PA72&output=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the main text is in French, but the key part we are interested in can be roughly translated as\n",
    "\n",
    "\"In most matters where we take measures data through observation, the most accurate results they can offer, it is almost always leads to a system of equations of the form\n",
    "$$E = a + bx + cy + fz + etc .$$\n",
    "where a, b, c, f etc are the known coefficients and  x , y, z etc are unknown and must be determined by the condition that the value of E is reduced, for each equation, to an amount or zero or very small.\"\n",
    "\n",
    "He continues\n",
    "\n",
    "\"Of all the principles that we can offer for this item, I think it is not broader, more accurate, nor easier than the one we have used in previous research application, and that is to make the minimum sum of the squares of the errors. By this means, it is between the errors a kind of balance that prevents extreme to prevail, is very specific to make known the state of the closest to the truth system. The sum of the squares of the errors $E^2 + \\left.E^\\prime\\right.^2 + \\left.E^{\\prime\\prime}\\right.^2 + etc$ being\n",
    "\\begin{align*}   &(a + bx + cy + fz + etc)^2 \\\\\n",
    "+ &(a^\\prime + b^\\prime x + c^\\prime y + f^\\prime z + etc ) ^2\\\\\n",
    "+ &(a^{\\prime\\prime} + b^{\\prime\\prime}x  + c^{\\prime\\prime}y +  f^{\\prime\\prime}z + etc )^2 \\\\\n",
    "+ & etc\n",
    "\\end{align*}\n",
    "if we wanted a minimum, by varying x alone, we will have the equation ...\"\n",
    "\n",
    "This is the earliest know printed version of the problem of least squares. The notation, however, is a little awkward for mordern eyes. In particular Legendre doesn't make use of the sum sign,\n",
    "$$\n",
    "\\sum_{i=1}^3 z_i = z_1 + z_2 + z_3\n",
    "$$\n",
    "nor does he make use of the inner product. \n",
    "\n",
    "In our notation, if we were to do linear regression, we would need to substitute:\n",
    "\\begin{align*}\n",
    "a &\\leftarrow y_1-c, \\\\ a^\\prime &\\leftarrow y_2-c,\\\\ a^{\\prime\\prime} &\\leftarrow y_3 -c,\\\\ \n",
    "\\text{etc.} \n",
    "\\end{align*}\n",
    "to introduce the data observations $\\{y_i\\}_{i=1}^{n}$ alongside $c$, the offset. We would then introduce the input locations\n",
    "\\begin{align*}\n",
    "b & \\leftarrow x_1,\\\\\n",
    "b^\\prime & \\leftarrow x_2,\\\\\n",
    "b^{\\prime\\prime} & \\leftarrow x_3,\\\\\n",
    "\\text{etc.}\n",
    "\\end{align*}\n",
    "and finally the gradient of the function\n",
    "$$x \\leftarrow -m.$$\n",
    "The remaining coefficients ($c$ and $f$) would then be zero. That would give us \n",
    "\\begin{align*}   &(y_1 - (mx_1+c))^2 \\\\\n",
    "+ &(y_2 -(mx_2 + c))^2\\\\\n",
    "+ &(y_3 -(mx_3 + c))^2 \\\\\n",
    "+ & \\text{etc.}\n",
    "\\end{align*}\n",
    "which we would write in the modern notation for sums as\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i-(mx_i + c))^2\n",
    "$$\n",
    "which is recognised as the sum of squares error for a linear regression.\n",
    "\n",
    "This shows the advantage of modern [summation operator](http://en.wikipedia.org/wiki/Summation), $\\sum$,  in keeping our mathematical notation compact. Whilst it may look more complicated the first time you see it, understanding the mathematical rules that go around it, allows us to go much further with the notation.\n",
    "\n",
    "Inner products (or [dot products](http://en.wikipedia.org/wiki/Dot_product)) are similar. They allow us to write\n",
    "$$\n",
    "\\sum_{i=1}^q u_i v_i\n",
    "$$\n",
    "in a more compact notation,\n",
    "$\n",
    "\\mathbf{u}\\cdot\\mathbf{v}.\n",
    "$\n",
    "\n",
    "Here we are using bold face to represent vectors, and we assume that the individual elements of a vector $\\mathbf{z}$ are given as a series of scalars\n",
    "$$\n",
    "\\mathbf{z} = \\begin{bmatrix} z_1\\\\ z_2\\\\ \\vdots\\\\ z_n \\end{bmatrix}\n",
    "$$\n",
    "which are each indexed by their position in the vector.\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "Linear algebra provides a very similar role, when we introduce [linear algebra](http://en.wikipedia.org/wiki/Linear_algebra), it is because we are faced with a large number of addition and multiplication operations. These operations need to be done together and would be very tedious to write down as a group. So the first reason we reach for linear algebra is for a more compact representation of our mathematical formulae. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Example: Olympic Marathons\n",
    "\n",
    "Now we will load in the Olympic marathon data. This is data of the olympic marathon times for the men's marathon from the first olympics in 1896 up until the London 2012 olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/olympic_marathon_men.csv', header=None, encoding= 'unicode_escape')\n",
    "x = np.array(data.iloc[:, 0].values).reshape(-1,1)\n",
    "y = np.array(data.iloc[:, 1].values).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see what these values are by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.47083333],\n",
       "       [4.46472926],\n",
       "       [5.22208333],\n",
       "       [4.15467867],\n",
       "       [3.90331675],\n",
       "       [3.56951267],\n",
       "       [3.82454477],\n",
       "       [3.62483707],\n",
       "       [3.59284275],\n",
       "       [3.53880792],\n",
       "       [3.67010309],\n",
       "       [3.39029111],\n",
       "       [3.43642612],\n",
       "       [3.20583007],\n",
       "       [3.13275665],\n",
       "       [3.32819844],\n",
       "       [3.13583758],\n",
       "       [3.0789588 ],\n",
       "       [3.10581822],\n",
       "       [3.06552909],\n",
       "       [3.09357349],\n",
       "       [3.16111704],\n",
       "       [3.14255244],\n",
       "       [3.08527867],\n",
       "       [3.10265829],\n",
       "       [2.99877553],\n",
       "       [3.03392977]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1896]\n",
      " [1900]\n",
      " [1904]\n",
      " [1908]\n",
      " [1912]\n",
      " [1920]\n",
      " [1924]\n",
      " [1928]\n",
      " [1932]\n",
      " [1936]\n",
      " [1948]\n",
      " [1952]\n",
      " [1956]\n",
      " [1960]\n",
      " [1964]\n",
      " [1968]\n",
      " [1972]\n",
      " [1976]\n",
      " [1980]\n",
      " [1984]\n",
      " [1988]\n",
      " [1992]\n",
      " [1996]\n",
      " [2000]\n",
      " [2004]\n",
      " [2008]\n",
      " [2012]]\n",
      "[[4.47083333]\n",
      " [4.46472926]\n",
      " [5.22208333]\n",
      " [4.15467867]\n",
      " [3.90331675]\n",
      " [3.56951267]\n",
      " [3.82454477]\n",
      " [3.62483707]\n",
      " [3.59284275]\n",
      " [3.53880792]\n",
      " [3.67010309]\n",
      " [3.39029111]\n",
      " [3.43642612]\n",
      " [3.20583007]\n",
      " [3.13275665]\n",
      " [3.32819844]\n",
      " [3.13583758]\n",
      " [3.0789588 ]\n",
      " [3.10581822]\n",
      " [3.06552909]\n",
      " [3.09357349]\n",
      " [3.16111704]\n",
      " [3.14255244]\n",
      " [3.08527867]\n",
      " [3.10265829]\n",
      " [2.99877553]\n",
      " [3.03392977]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that they are not `pandas` data frames for this example, they are just arrays of dimensionality $n\\times 1$, where $n$ is the number of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to have you coding linear regression in python. We will do it in two ways, once using iterative updates (coordinate ascent) and then using linear algebra. The linear algebra approach will not only work much better, it is easy to extend to multiple input linear regression and *non-linear* regression using basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data\n",
    "\n",
    "You can make a plot of $y$ vs $x$ with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pace in min/km')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import pylab as plt\n",
    "\n",
    "plt.plot(x, y, 'rx')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('pace in min/km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood: Iterative Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take the maximum likelihood approach we studied in the lecture to fit a line, $y_i=mx_i + c$, to the data you've plotted. We are trying to minimize the error function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(m, c) =  \\sum_{i=1}^n(y_i-mx_i-c)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with respect to $m$ and $c$. We can start with an initial guess for $m$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = -0.4\n",
    "c = 80 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the maximum likelihood update to find an estimate for the offset, $c$.\n",
    "\n",
    "### Coordinate Descent\n",
    "\n",
    "In the lecture, we saw how the steepest decent algorithm works. Here, we explain another approach. It is known as *coordinate descent*. In coordinate descent, we choose to move one parameter at a time. Ideally, we design an algorithm that at each step moves the parameter to its minimum value. At each step we choose to move the individual parameter to its minimum.\n",
    "\n",
    "To find the minimum, we look for the point in the curve where the gradient is zero. This can be found by taking the gradient of $E(m,c)$ with respect to the parameter. \n",
    "\n",
    "#### Update for Offset\n",
    "\n",
    "Let's consider the parameter $c$ first. The gradient goes nicely through the summation operator, and we obtain\n",
    "$$\n",
    "\\frac{\\text{d}E(m,c)}{\\text{d}c} = -\\sum_{i=1}^n 2(y_i-mx_i-c).\n",
    "$$\n",
    "Now we want the point that is a minimum. A minimum is an example of a [*stationary point*](http://en.wikipedia.org/wiki/Stationary_point), the stationary points are those points of the function where the gradient is zero. They are found by solving the equation for $\\frac{\\text{d}E(m,c)}{\\text{d}c} = 0$. Substituting in to our gradient, we can obtain the following equation, \n",
    "$$\n",
    "0 = -\\sum_{i=1}^n 2(y_i-mx_i-c)\n",
    "$$\n",
    "which can be reorganised as follows,\n",
    "$$\n",
    "c^* = \\frac{\\sum_{i=1}^n(y_i-m^*x_i)}{n}.\n",
    "$$\n",
    "The fact that the stationary point is easily extracted in this manner implies that the solution is *unique*. There is only one stationary point for this system. Traditionally when trying to determine the type of stationary point we have encountered we now compute the *second derivative*,\n",
    "$$\n",
    "\\frac{\\text{d}^2E(m,c)}{\\text{d}c^2} = 2n.\n",
    "$$\n",
    "The second derivative is positive, which in turn implies that we have found a minimum of the function. This means that setting $c$ in this way will take us to the lowest point along that axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786.0197711451852\n"
     ]
    }
   ],
   "source": [
    "# set c to the minimum\n",
    "c = (y - m*x).mean()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update for Slope\n",
    "\n",
    "Now we have the offset set to the minimum value, in coordinate descent, the next step is to optimise another parameter. Only one further parameter remains. That is the slope of the system. \n",
    "\n",
    "Now we can turn our attention to the slope. We once again peform the same set of computations to find the minima. We end up with an update equation of the following form.\n",
    "\n",
    "$$m^* = \\frac{\\sum_{i=1}^n (y_i - c)x_i}{\\sum_{i=1}^n x_i^2}$$\n",
    "\n",
    "Communication of mathematics in data science is an essential skill, in a moment, you will be asked to rederive the equation above. Before we do that, however, we will briefly review how to write mathematics in the notebook.\n",
    "\n",
    "### $\\LaTeX$ for Maths\n",
    "\n",
    "These cells use [Markdown format](http://en.wikipedia.org/wiki/Markdown). You can include maths in your markdown using [$\\LaTeX$ syntax](http://en.wikipedia.org/wiki/LaTeX), all you have to do is write your answer inside dollar signs, as follows:\n",
    "\n",
    "To write a fraction, we write `$\\frac{a}{b}$`, and it will display like this $\\frac{a}{b}$. To write a subscript we write `$a_b$` which will appear as $a_b$. To write a superscript (for example in a polynomial) we write `$a^b$` which will appear as $a^b$. There are lots of other macros as well, for example we can do greek letters such as `$\\alpha, \\beta, \\gamma$` rendering as $\\alpha, \\beta, \\gamma$. And we can do sum and intergral signs as `$\\sum \\int \\int$`.\n",
    "\n",
    "You can combine many of these operations together for composing expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Convert the following python code expressions into $\\LaTeX$j, writing your answers below. In each case write your answer as a single equality (i.e. your maths should only contain one expression, not several lines of expressions). For the purposes of your $\\LaTeX$ please assume that `x` and `w` are $n$ dimensional vectors. \n",
    "\n",
    "(a) \n",
    "``` python\n",
    "f = x.sum()\n",
    "```\n",
    "\n",
    "(b) \n",
    "``` python \n",
    "m = x.mean()\n",
    "```\n",
    "\n",
    "(c) \n",
    "``` python\n",
    "g = (x*w).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "ÔºàaÔºâ $f = x.sum()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient With Respect to the Slope\n",
    "Now that you've had a little training in writing maths with $\\LaTeX$, we will be able to use it to answer questions. The next thing we are going to do is a little differentiation practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Derive the the gradient of the objective function with respect to the slope, $m$. Rearrange it to show that the update equation written above does find the stationary points of the objective function. By computing its derivative show that it's a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "\n",
    "$E(m,c)=\\sum_{i=1}^n (y_i-m x_i-c)^2$\n",
    "\n",
    "$\\frac{d}{dm}E(m,c)=-2\\sum_{i=1}^n x_i(y_i-m x_i-c)$\n",
    "\n",
    "$0=-2\\sum_{i=1}^n (y_i-m x_i-c)$\n",
    "\n",
    "$0=2m\\sum_{i=1}^n x_i^2-2\\sum_{i=1}^n x_i (y_i-c)$\n",
    "\n",
    "$2\\sum_{i=1}^n x_i(y_i-c)=2m\\sum_{i=1}^n x_i^2$\n",
    "\n",
    "$m=\\frac{\\sum_{i=1}^n x_i (y_i-c)}{\\sum_{i=1}^n x_i^2}$\n",
    "\n",
    "second derivative\n",
    "\n",
    "$\\frac{d}{dm}E(m,c)=-2\\sum_{i=1}^n x_i(y_i-mx_i-c)$\n",
    "\n",
    "$\\frac{d}{dm}E(m,c)=2m\\sum_{i=1}^n x_i^2-2\\sum_{i=1}^n x_i(y_i-c)$\n",
    "\n",
    "$\\frac{d^2}{dm^2}E(m,c)=2\\sum_{i=1}^n x_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at how good our fit is by computing the prediction across the input space. First create a vector of 'test points',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(1890, 2020, 130)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this vector to compute some test predictions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = m*x_test + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot those test predictions with a blue line on the same plot as the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b9d7668>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit isn't very good, we need to iterate between these parameter updates in a loop to improve the fit, we have to do this several times,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3987259642504537\n",
      "783.5273797269986\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    m = ((y - c)*x).sum()/(x*x).sum()\n",
    "    c = (y-m*x).sum()/y.shape[0]\n",
    "print(m)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try plotting the result again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bafc198>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8VfP+x/HXp0lmDSdTrnINt05p2o1kSJQyRLiZboi4uGYqhK7hqtxr1hUhdCVjyJTKTHXSoIGKiyJknq4on98f39XPkVNnn/aw9vB+Ph77cfZZe52zPi3Oe6+91nd9vubuiIhI4asWdwEiIpIdCnwRkSKhwBcRKRIKfBGRIqHAFxEpEgp8EZEiocAXESkSCnwRkSKhwBcRKRI14i6gvPr163ujRo3iLkNEJK/MmDHjM3cvqWy9nAr8Ro0aUVZWFncZIiJ5xczeT2Y9ndIRESkSCnwRkSKhwBcRKRIKfBGRIpFy4JtZbTObZmazzWyemQ2Jljc2s6lmtsjM7jezWqmXKyIi6ysdR/grgC7u3gJoCXQ3sw7AUOBad98J+BLol4ZtiYjIeko58D34Lvq2ZvRwoAvwYLR8NNAr1W2JiMj6S8s5fDOrbmazgE+BicA7wFfuvjJaZSmwbTq2VZHly+Hss+HrrzO1BRGR/JeWwHf3Ve7eEmgItAOaVLRaRT9rZv3NrMzMypYvX75e2580CW64AZo2hccfX69fISJS8NI6SsfdvwKeBzoAW5jZ6jt5GwIfreVnRrp7wt0TJSWV3hlcoT594PXXoV49OOggOOqocNQvIiK/SsconRIz2yJ6viHQFVgATAEOi1brC4xPdVvr0rYtlJXBkCHw4IPQpAn85z/gFX6uEBEpPuk4wt8amGJmc4DpwER3fwIYAJxjZouBesCoNGxrnWrVgksugZkzYccd4eij4cADYcmSTG9ZRCT3mefQIXAikfB0NU9btSqc17/oIqhRA4YPh5NOgmq61UxECoyZzXD3RGXrFWz8Va8eRu7MnRtO95xyCnTpAosWxV2ZiEg8CjbwV9thB3juObj9dpg1C3bdFa65BlaurPxnRUQKScEHPoAZ9OsH8+dDt25w/vnQsSPMmRN3ZSIi2VMUgb/aNtvAI4/A/ffD++9DmzbhIu+KFXFXJiKSeUUV+BCO9o84AhYsCOP3L78cWrcO4/hFRApZ0QX+avXqwT33wIQJ8M030KlTuMj7/fdxVyYikhlFG/ir9egB8+aFUTzXXQfNm4dWDSIihaboAx9gs83gllvghRfCmP2uXeHEE+Grr+KuTEQkfRT45eyxB8yeDQMGwF13hWZsjz4ad1UiIumhwF/DhhvC1VfD1KnQoAEccki4yPvJJ3FXJiKSGgX+WrRpA9OnwxVXwPjx4Wj/nnvUjE1E8pcCfx1q1gy9eGbNgl12gb/8BXr2hA8+iLsyEZGqU+AnoUkTeOkluP76cGG3tDRc5P3ll7grExFJngI/SdWrwxlnhGZsHTvCaafBXnvBwoVxVyYikhwFfhU1bgzPPAN33glvvhmasQ0dqmZsIpL7FPjrwQyOOy40Y+vRAwYOhPbtw7l+EZFcpcBPwdZbw8MPhykVP/wQEolwkffHH+OuTETk9xT4adC7dzjaP+YYuOoqaNUKXn017qpERH5LgZ8mdeuGu3Offhp++AF23z1c5P3uu7grExEJFPhp1q1bGMlz2mlw003QrBk8+2zcVYmIKPAzYtNN4cYb4cUXoXbt8CZw/PHwxRdxVyYixUyBn0G77x5G7gwaFNoyNG0KDz0Ud1UiUqwU+BlWu3a4kDt9ehjVc9hh4fHxx3FXJiLFRoGfJa1awbRpIfyfeCIc7d91l5qxiUj2KPCzqGbNcHpn1qwQ+McfD927w3vvxV2ZiBQDBX4M/vSncEH3ppvCeP1mzcJFXjVjE5FMUuDHpFq1MHRz7txfx+zvsQe89VbclYlIoVLgx2z77eGpp2D06HC3bosW4Tz/zz/HXZmIFBoFfg4wC5OrLFgABx0U+vG0awdvvBF3ZSJSSFIOfDPbzsymmNkCM5tnZmdGy+ua2UQzWxR9rZN6uYVtyy3hgQdCQ7aPPw6hP2gQ/O9/cVcmIoUgHUf4K4Fz3b0J0AE4zcyaAgOBSe6+EzAp+l6ScMgh4fRO375hQvWWLeHll+OuSkTyXcqB7+7L3P2N6Pm3wAJgW+BgYHS02migV6rbKiZ16sCoUTBxIvz0E3TuDKefDt9+G3dlIpKv0noO38waAa2AqcCW7r4MwpsC0CCd2yoWXbuGmbXOPDPMo1taGi7yiohUVdoC38w2AR4CznL3b6rwc/3NrMzMypYvX56ucgrKJpvAddfBK6+E5z16hIu8n38ed2Uikk/SEvhmVpMQ9mPc/eFo8SdmtnX0+tbApxX9rLuPdPeEuydKSkrSUU7B6tgRZs6Eiy+G++4Ld+s+8IDaM4hIctIxSseAUcACd/9XuZceA/pGz/sC41PdlsAGG8Dll0NZGWy3HRxxBBx6KCxbFndlIpLr0nGEvxtwLNDFzGZFjx7A1cC+ZrYI2Df6XtKkRQt4/XUYNizMstWkCdxxh472RWTtzHMoIRKJhJeVlcVdRt5ZuBBOOin05+naFW69FXbYIe6qRCRbzGyGuycqW0932haAnXeGKVNgxAiYOhWaNw8XeVetirsyEcklCvwCUa0anHIKzJsHe+4JZ58dmrLNnx93ZSKSKxT4BWa77WDCBLj3Xli0KEy8cvnl4eYtESluCvwCZAZHHx2O7g89FC65BNq2DSN7RKR4KfALWIMGYbz++PHw2WfQvj1ccIGasYkUKwV+ETjooHBuv18/GD4cdt0VXngh7qpEJNsU+EViiy1g5EiYNClMpbjXXvDXv8I3STfBEJF8p8AvMl26wJw5cM454Q2gtDRc5BWRwqfAL0Ibbwz//GeYQH3zzeGAA+CYY8J5fhEpXAr8Ita+fZhG8dJLYdy40J5h7Fi1ZxApVAr8IlerFlx2GcyYAY0bw5FHQq9e8OGHcVcmIummwBcgtGN47TW45powy1bTpnDbbTraFykkCnz5f9Wrw7nnhou6rVtD//6wzz7wzjtxVyYi6aDAl9/ZcccwfPPWW8OpnubN4V//UjM2kXynwJcKVasWjvDnzQtH+eeeC506wdy5cVcmIutLgS/r1LAhPPZYaNHw7rvhVM+QIWrGJpKPFPhSKTPo0wcWLIDDDw+jetq0gWnT4q5MRKpCgS9Jq18fxoyBxx+HL78Mk6qfey788EPclYlIMhT4UmUHHBDO7Z90UriY27x5mHFLRHKbAl/Wy+abw7//HYK+WrXQo6d/f/j667grE5G1UeBLSvbaC2bPhvPPh1Gjwg1bjz8ed1UiUhEFvqRso41g2LAwgXq9eqH//pFHwvLlcVcmIuUp8CVtEokwjeLf/w4PPRSasY0Zo/YMIrlCgS9pVasWDB4MM2eGO3aPOQYOPBCWLIm7MhFR4EtGlJbCK6/AtdeGC7ulpeEi7y+/xF2ZSPFS4EvGVK8OZ50Fb74J7dqFKRW7dIFFi+KuTKQ4KfAl43bYIbRcHjUKZs0Kk6gPHw4rV8ZdmUhxUeBLVpjBCSfA/PnQrRtccEG4U3fOnLgrEykeCnzJqm22gUceCVMqfvBB6MlzySWwYkXclYkUPgW+ZJ1ZaMI2f34Yr3/55dCqVZhxS0QyJy2Bb2Z3mNmnZja33LK6ZjbRzBZFX+ukY1tSOOrVg7vvhiefhO++g912Cxd5v/8+7spEClO6jvDvArqvsWwgMMnddwImRd+L/M7++4dmbKeeCtdfD82awXPPxV2VSOFJS+C7+4vAF2ssPhgYHT0fDfRKx7akMG26Kdx0E7z4ItSsCfvuC/36wVdfxV2ZSOHI5Dn8Ld19GUD0tUFFK5lZfzMrM7Oy5Wq+UvQ6dw7N2AYOhNGjQzO2Rx+NuyqRwhD7RVt3H+nuCXdPlJSUxF2O5IANN4R//CM0Y2vQAA45BI44Aj75JO7KRPJbJgP/EzPbGiD6+mkGtyUFqE0bmD4drrwSxo8PzdjuvlvN2ETWVyYD/zGgb/S8LzA+g9uSAlWzJlx4YbhDt0kT6NsXevQIY/hFpGrSNSzzPuA1YBczW2pm/YCrgX3NbBGwb/S9yHpp0gReegluuCF8LS2Fm29WMzaRqjDPoc/HiUTCy8rK4i5Dctx774XpFCdOhN13h9tvh112ibsqkfiY2Qx3T1S2XuwXbUWqqlEjeOYZuPNOmDsXWrSAq6+Gn3+OuzKR3KbAl7xkBscdBwsWQM+eMGgQtG8fJl4RkYop8CWvbbVVmE7xwQfho4+gbVu46CL48ce4KxPJPQp8KQi9e4dmbMceC1ddBS1bhhm3RORXCnwpGHXrhvP6zzwTjvA7d4YzzgiN2UREgS8FaL/9wsXc008P/XmaNYNnn427KpH4KfClIG2yya9j9mvXDrNsHX88fLFmiz+RIqLAl4K2227hLt0LL4R77gnN2B56KO6qROKhwJeCV7t26MdTVhamWDzssHCRd9myuCsTyS4FfqqGDYMpU367bMqUsFxySsuWMG1auElrwoRwtH/XXWrGJsVDgZ+qtm1D797VoT9lSvi+bdt465IK1agBAwaEnvvNmoXz+t26hXYNIoVOgZ+qvfeGceNCyF9ySfg6blxYLjlrl13ghRdCA7bXXgvhf+ONasYmha04Az/Z0zDJrrf33vDXv8Lll4evCvu8UK1amEd37txfx+x37hzaNYgUouIM/GRPwyS73pQpMGIEDB4cvq75JiE5bfvt4cknw+Qqb70VzvVfeaWasUkBcvecebRp08azZvJk9/r13QcPDl8nT16/9Va/vnr5mt+vNnRoxT87dGh6/j2SFh9/7H7EEe7g3qKF+4wZcVckUjmgzJPI2OI8wofkT8NUtt706b89Z7/6nP706b9dL5lPC9ke8VPoI4zW49+35ZZw//3wyCNhDt127cKE6v/7X4ZrFcmGZN4VsvXIyyP8dG4z2U8LyUjmE0Uy20v2k0ky62X7U06Kn76+v2yo9+sXjvaH1R/qs6/TJzTJTSR5hB97yJd/ZC3wkw2CdAbwaoMHh90+ePC6a0v1Daaq/8ZU34SSWS8T+7MyyezPSuqaONG9z5aT/VPq+7UHT/avv15L7dl+c8zn04Tp3FeiwF+nuP5nSzbMK3tTyPb20vlpKJ2fmJKVzP6spK7vvnO/qXcU+psN9hWb5cCbYzqvH6XzDSZdny6TXS/btefgm5ACP9ek64i7qtIV5sm+CSWzXrre0JJRlf2ZRF1Ljg/rDGGwH3us+2efref20vXmmIZPMGldZ33Wy8bAiWy/GWeZAj/XpPOoJ1np+ENJ5vdUZb1sHuFXZX9WofafBw327zaq7/tUm+wlJe733+/+yy/l1sv2m2MaPsGkdZ2qrJeufZXt2rP5/3ESFPj5KJ0fFdP1UTifj4yS3Z/rWfvPdep7/50nO7j36uX+4YeefBBkO4Dds/sGk8x6cbx55Osn1Uoo8Itdut48Cvzcp7unVPvKfwz1YcPca9d2P2Djyf7DJvX9l0lZenPM0CeYrBwlp/NAItu1V+V3ZYkCXySLFi50H9F4qO/FZN9nH/d33oleyOSbY4Y/wWT0PHi2LwDn8yfVJCjwRbJs1Sr3ESPcN93UfaON3K+91n3lyrir8twcpZNOGqWTdOBbWDc3JBIJLysri7sMkZQsWRJuyp4wAdq3h1GjoLQ07qqkkJnZDHdPVLZe8bZWEMmQ7baDxx+HMWNg8WJo1Sp05vjpp7grk2KnwBfJADM46qjQarl37zBVQiLx+xZLItmkwBfJoJISuO8+GD8ePv8cOnSACy6AH36IuzIpRhkPfDPrbmZvm9liMxuY6e2J5KKDDoL586FfPxg+HFq0gOefj7sqKTYZDXwzqw7cDOwPNAWONLOmmdymSK7afHMYORImTQpTKe69N5xyCnz9ddyVSbHI9BF+O2Cxu7/r7j8BY4GDM7xNkZzWpQu8+Sacey7cdlsYwTNhQtxVSTHIdOBvCywp9/3SaNn/M7P+ZlZmZmXLly/PcDkiuWGjjeCaa8IE6nXqwAEHwNFHg/4EJJMyHfhWwbLfDPx395HunnD3RElJSYbLEckt7drBjBlw2WXwwAPQtCmMHQs5dHuMFJBMB/5SYLty3zcEPsrwNkXySq1acOml8MYbsMMOcOSRcPDB8OGHcVcmhSbTgT8d2MnMGptZLaAP8FiGtymSl5o1g1dfhX/+E557LhztjxwZLvCKpENGA9/dVwKnA88AC4Bx7j4vk9sUyWfVq8M554SLum3awMknwz77hDt2RVKV8XH47v6ku+/s7n909yszvT2RQvDHP4bhm7fdFk717LprOPJftSruyiSf6U5bkRxlBieeGG7Y6toVzjsPOnaEuXPjrkzylQJfJMdtu21ozTB2LLz3HrRuHUb1qBmbVJUCXyQPmMGf/xyO9o84AoYMCcE/dWrclUk+UeCL5JH69eHee+GJJ0JLho4dw0Xe77+PuzLJBwp8kTzUsyfMmxd68Vx7bbioO3ly3FVJrlPgi+SpzTaDW24JXTerVQvDN086Cb76Ku7KJFcp8EXy3J57wpw5oc/+HXeEZmyP6fZGqYACX6QAbLghDB0aLuLWqxdaM/TpA59+GndlkksU+CIFJJGAsrIwh+4jj4T2DGPGqBmbBAp8kQJTqxZcfDHMnAk77QTHHBPaLy9ZUvnPSmFT4IsUqKZN4eWX4brrwoXd0lIYMULN2IqZAl+kgFWvDmeeGdoxtG8Pp54aplZctCjuyiQOCnyRItC4MTz7LIwaBbNnh3H7w4bBypVxVybZpMAXKRJmcMIJoT1D9+4wYAB06BDeAKQ4KPBFisw228DDD8O4ceFCbiIBgwfDihVxVyaZpsAXKUJmcPjh4Wj/qKPgiiugVaswqboULgW+SBGrVw9Gj4anngoN2HbbDc46C777Lu7KJBMU+CJC9+5hJM+pp8L110Pz5jBxYtxVSbop8EUEgE03hZtughdfDDdv7bcf9OsHX34Zd2WSLgp8EfmNzp3DyJ2BA8PpnqZNQ5sGyX8KfBH5ndq14R//gGnTYKut4NBDw0xbn3wSd2WSCgW+iKxV69Yh9K+6KrRcbtIE7r5bzdjylQJfRNapZk0YNAhmzQqB37cv7L8/vP9+3JVJVSnwRSQpf/oTvPQS3HhjaMrWrBncfLOaseUTBb6IJK1aNTj99DCEs1On8HzPPeHtt+OuTJKhwBeRKmvUCJ5+Gu66K0ym3qIFXH01/Pxz3JXJuijwRWS9mIXz+fPnw4EHhvP87duHiVckNynwRSQlW20FDzwADz0EH30EbdvChRfCjz/GXZmsSYEvImlx6KGwYAH85S9hDH/LlvDKK3FXJeWlFPhmdriZzTOzX8wsscZrg8xssZm9bWbdUitTRPJBnTpwxx3wzDPhCL9zZ/jb3+Dbb+OuTCD1I/y5wKHAi+UXmllToA9QCnQHbjGz6iluS0TyxH77hZE8f/tbGLrZrFl4E5B4pRT47r7A3SsakHUwMNbdV7j7f4HFQLtUtiUi+WWTTULnzZdfho02Ch05jzsOvvgi7sqKV6bO4W8LLCn3/dJomYgUmU6dwsidiy6CMWPC3boPPhh3VcWp0sA3s+fMbG4Fj4PX9WMVLKuw+4aZ9TezMjMrW758ebJ1i0geqV07zKo1fTo0bBhm2+rdG5Yti7uy4lJp4Lt7V3dvVsFj/Dp+bCmwXbnvGwIfreX3j3T3hLsnSkpKqla9iOSVli1h6tRwk9aECaH18p13qhlbtmTqlM5jQB8z28DMGgM7AdMytC0RySM1asCAATBnTphZ64QToFs3eO+9uCsrfKkOyzzEzJYCHYEJZvYMgLvPA8YB84GngdPcfVWqxYpI4dh5Z3j++TCK57XXwkieG26AVUqKjDHPoc9SiUTCy8rK4i5DRLLsgw/glFPCZOodO8KoUeHiriTHzGa4e6Ky9XSnrYjE7g9/COf077kndN5s2RKuvFLN2NJNgS8iOcEMjjkmtGfo1QsuvhgSCZgxI+7KCocCX0RySoMGcP/9YeL05ctDB86BA+F//4u7svynwBeRnNSrV2i9fNxxMHRo6Ln/4ouV/pisgwJfRHLWFlvA7bfDc8/BypVhdq3TToNvvom7svykwBeRnLfPPvDmm3D22TBiRBjC+eSTcVeVfxT4IpIXNt4Y/vUvePVV2HRT6NkTjj0WPvss7sryhwJfRPJKhw7wxhtwySUwdmxozzBunNozJEOBLyJ5Z4MNYMiQMGRz++3hz3+GQw4JUyzK2inwRSRv7bpraMswfHiYYKVp03CXro72K6bAF5G8VqMGnHdeuKjbsiWceCJ07Qrvvht3ZblHgS8iBWHHHWHyZLj11tB3v1kzuPZaNWMrT4EvIgWjWjXo3z/csNWlC5xzDuy2G8ybF3dluUGBLyIFp2FDePxx+M9/4J13oFUr+Pvf4aef4q4sXgp8ESlIZnDkkeFo/7DD4NJLQzO26dPjriw+CnwRKWglJeFI/7HH4Isvwjj+88+HH36Iu7LsU+CLSFE48MBwLv+kk+Caa8KQzuefj7uq7FLgi0jR2Hxz+Pe/w2gegL33hpNPhq+/jreubFHgi0jR2XvvMIn6eeeFbpylpfDEE3FXlXkKfBEpShttFO7Qfe01qFMnnPI56qgw6UqhUuCLSFFr1y705BkyBB58MLRnuO++wmzPoMAXkaJXq1bovjlzJvzxj+FI/6CDYOnSuCtLLwW+iEiktBReeSX03Z80KXw/ciT88kvclaWHAl9EpJzq1cPMWnPnhhu1Tj45zLi1eHHclaVOgS8iUoEddghz6d52W5hwpXnzMH5/5cq4K1t/CnwRkbUwC+2W58+H/fYLd+h26hRaMecjBb6ISCW23RYefTRMqfjee9C6dejNs2JF3JVVjQJfRCQJZmEqxfnzoU+f0H2zTRuYOjXuypKnwBcRqYL69eGee2DChNCSoWPH0Hf/++/jrqxyKQW+mQ03s7fMbI6ZPWJmW5R7bZCZLTazt82sW+qliojkjh49QjO2U04JM2s1bx6GcuayVI/wJwLN3H1XYCEwCMDMmgJ9gFKgO3CLmVVPcVsiIjlls83gllvghRfC3Lpdu4ZunF99FXdlFUsp8N39WXdfPUjpdaBh9PxgYKy7r3D3/wKLgXapbEtEJFftsQfMng0XXAB33BHaM4wfH3dVv5fOc/gnAE9Fz7cFlpR7bWm0TESkIG24IQwdGi7ilpRAr17h4u6nn8Zd2a8qDXwze87M5lbwOLjcOhcBK4ExqxdV8KsqbEVkZv3NrMzMypYXcps6ESkKiQSUlcEVV8Ajj0CTJnDvvbnRjK3SwHf3ru7erILHeAAz6wscABzt/v//pKXAduV+TUPgo7X8/pHunnD3RElJSWr/GhGRHFCzJlx0EcyaBbvsAsceCz17wgcfxFtXqqN0ugMDgIPcvfwMkY8BfcxsAzNrDOwETEtlWyIi+aZJE3jpJbj++nBht7QURoyIrxlbqufwbwI2BSaa2Swz+zeAu88DxgHzgaeB09x9VYrbEhHJO9WrwxlnhGZsHTrAqafCXnvBwoXZr8U8F04sRRKJhJeVlcVdhohIRrjDXXeFG7V+/DFMunLOOWFIZyrMbIa7JypbT3faiohkiRkcf3xoz7D//jBgALRvH4Z0ZoMCX0Qky7beGh5+OEyp+OGHYWTPdddlfrsKfBGRmPTuHY72jz46TK2YaSmeORIRkVTUrRvO62eDjvBFRIqEAl9EpEgo8EVEioQCX0SkSCjwRUSKhAJfRKRIKPBFRIqEAl9EpEjkVPM0M1sOvB93HWtRH/gs7iLWU77Wnq91g2qPS7HWvr27VzqhSE4Ffi4zs7JkutHlonytPV/rBtUeF9W+bjqlIyJSJBT4IiJFQoGfvJFxF5CCfK09X+sG1R4X1b4OOocvIlIkdIQvIlIkijbwzewOM/vUzOaWW9bCzF4zszfN7HEz26zca4PMbLGZvW1m3cot7x4tW2xmA3OtdjPb18xmRMtnmFmXcj/TJlq+2MxuMDPLpdrLvf4HM/vOzM4rtyyn93v02q7Ra/Oi12tHy3N6v5tZTTMbHS1fYGaDyv1MVve7mW1nZlOiOuaZ2ZnR8rpmNtHMFkVf60TLLdqni81sjpm1Lve7+kbrLzKzvjlY+9FRzXPM7FUza1Hud6Vnv7t7UT6APYDWwNxyy6YDe0bPTwAuj543BWYDGwCNgXeA6tHjHWAHoFa0TtMcq70VsE30vBnwYbmfmQZ0BAx4Ctg/l2ov9/pDwAPAedH3+bDfawBzgBbR9/WA6vmw34GjgLHR842A94BGcex3YGugdfR8U2Bh9Pc4DBgYLR8IDI2e94j2qQEdgKnR8rrAu9HXOtHzOjlWe6fVNQH7l6s9bfu9aI/w3f1F4Is1Fu8CvBg9nwj0jp4fTPgDWOHu/wUWA+2ix2J3f9fdfwLGRuvmTO3uPtPdP4qWzwNqm9kGZrY1sJm7v+bh/6q7gV65VDuAmfUi/HHOK7d+zu93YD9gjrvPjn72c3dflSf73YGNzawGsCHwE/ANMex3d1/m7m9Ez78FFgDbRtsdHa02ml/34cHA3R68DmwR7fNuwER3/8Ldv4z+vd1zqXZ3fzWqDeB1oGH0PG37vWgDfy3mAgdFzw8HtouebwssKbfe0mjZ2pbHYW21l9cbmOnuKwh1Li33Ws7VbmYbAwOAIWusnw/7fWfAzewZM3vDzC6Iluf8fgceBL4HlgEfANe4+xfEvN/NrBHhE+tUYEt3XwYhWIEG0Wo5+beaZO3l9SN8UoE01q7A/60TgNPMbAbhI9hP0fKKzrH6OpbHYW21A2BmpcBQ4OTViyr4HblW+xDgWnf/bo3186H2GsDuwNHR10PMbB/yo/Z2wCpgG8IpzHPNbAdirN3MNiGc2jvL3b9Z16oVLIv1b7UKta9ef29C4A9YvaiC1dardk1iXo67v0X4KI6Z7Qz0jF5aym+PmBsCq0+TrG15Vq2jdsysIfAI8Bd3fydavJRfPzJCbtbeHjjMzIYBWwC/mNmPwAxyf78vBV5w98+i154knEO/l9zf70dG9QWDAAABtUlEQVQBT7v7z8CnZvYKkCAcZWZ9v5tZTUJgjnH3h6PFn5jZ1u6+LDpl82m0fG1/q0uBvdZY/nwm64Yq146Z7QrcTriu83m0eF35UzWZvGiR6w/ChajyF7EaRF+rEc6tnhB9X8pvL9q+S7iQUiN63phfL6aU5ljtW0R19a7gd0wnXNhaffGwRy7VvsbPXMavF23zYb/XAd4gXPSsATwH9MyH/U44srwzqm9jYD6waxz7ParhbuC6NZYP57cXPodFz3vy24u206LldYH/Rv9d6kTP6+ZY7X8gXB/stMb6advvGf+fLFcfwH2Ec5Q/E95B+wFnEq6kLwSuJroxLVr/IsKV8rcpN6qCMCpgYfTaRblWO3Ax4XzsrHKP1X/oCcJ53HeAm8r/e3Oh9jV+7jKiwM+H/R6tfwzhYvPc1X/U+bDfgU0Io6LmEcL+/Lj2O+F0mBNGPK3+/7cHYdTTJGBR9LVutL4BN0f1vQkkyv2uEwiBuhg4Pgdrvx34sty6Zene77rTVkSkSOiirYhIkVDgi4gUCQW+iEiRUOCLiBQJBb6ISJFQ4IuIFAkFvohIkVDgi4gUif8D8ML5b/ZV5EcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_test = m*x_test + c\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we need more iterations than 10! In the next question you will add more iterations and report on the error as optimisation proceeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "There is a problem here, we seem to need many interations to get to a good solution. Let's explore what's going on. Write code which alternates between updates of `c` and `m`. Include the following features in your code.\n",
    "\n",
    "(a) Initialise with `m=-0.4` and `c=80`.\n",
    "(b) Every 10 iterations compute the value of the objective function for the training data and print it to the screen (you'll find hints on this in the lab from last week.\n",
    "(c) Cause the code to stop running when the error change over less than 10 iterations is smaller than $1\\times10^{-4}$. This is known as a stopping criterion.\n",
    "\n",
    "Why do we need so many iterations to get to the solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative: 10 objective 25.082468011856147\n",
      "iterative: 20 objective 24.93061542610306\n",
      "iterative: 30 objective 24.779760967112086\n",
      "iterative: 40 objective 24.629898074197822\n",
      "iterative: 50 objective 24.481020229798286\n",
      "iterative: 60 objective 24.33312095919156\n",
      "iterative: 70 objective 24.186193830213902\n",
      "iterative: 80 objective 24.040232452980188\n",
      "iterative: 90 objective 23.89523047960615\n",
      "iterative: 100 objective 23.751181603931776\n",
      "iterative: 110 objective 23.608079561247553\n",
      "iterative: 120 objective 23.465918128022416\n",
      "iterative: 130 objective 23.324691121631865\n",
      "iterative: 140 objective 23.184392400090143\n",
      "iterative: 150 objective 23.04501586178281\n",
      "iterative: 160 objective 22.906555445201015\n",
      "iterative: 170 objective 22.76900512867836\n",
      "iterative: 180 objective 22.63235893012887\n",
      "iterative: 190 objective 22.496610906786987\n",
      "iterative: 200 objective 22.361755154948536\n",
      "iterative: 210 objective 22.227785809714717\n",
      "iterative: 220 objective 22.09469704473653\n",
      "iterative: 230 objective 21.96248307196144\n",
      "iterative: 240 objective 21.831138141382365\n",
      "iterative: 250 objective 21.70065654078605\n",
      "iterative: 260 objective 21.571032595506562\n",
      "iterative: 270 objective 21.44226066817668\n",
      "iterative: 280 objective 21.31433515848457\n",
      "iterative: 290 objective 21.187250502928293\n",
      "iterative: 300 objective 21.061001174575193\n",
      "iterative: 310 objective 20.93558168282119\n",
      "iterative: 320 objective 20.81098657315189\n",
      "iterative: 330 objective 20.68721042690543\n",
      "iterative: 340 objective 20.56424786103681\n",
      "iterative: 350 objective 20.442093527883493\n",
      "iterative: 360 objective 20.320742114933555\n",
      "iterative: 370 objective 20.20018834459388\n",
      "iterative: 380 objective 20.0804269739612\n",
      "iterative: 390 objective 19.961452794593693\n",
      "iterative: 400 objective 19.843260632284586\n",
      "iterative: 410 objective 19.725845346837335\n",
      "iterative: 420 objective 19.609201831841638\n",
      "iterative: 430 objective 19.493325014451766\n",
      "iterative: 440 objective 19.378209855165856\n",
      "iterative: 450 objective 19.26385134760661\n",
      "iterative: 460 objective 19.150244518303648\n",
      "iterative: 470 objective 19.037384426477217\n",
      "iterative: 480 objective 18.925266163823213\n",
      "iterative: 490 objective 18.813884854299854\n",
      "iterative: 500 objective 18.703235653915787\n",
      "iterative: 510 objective 18.593313750518828\n",
      "iterative: 520 objective 18.48411436358728\n",
      "iterative: 530 objective 18.375632744021836\n",
      "iterative: 540 objective 18.267864173938854\n",
      "iterative: 550 objective 18.160803966465487\n",
      "iterative: 560 objective 18.05444746553547\n",
      "iterative: 570 objective 17.948790045687236\n",
      "iterative: 580 objective 17.843827111861973\n",
      "iterative: 590 objective 17.739554099204405\n",
      "iterative: 600 objective 17.63596647286399\n",
      "iterative: 610 objective 17.53305972779771\n",
      "iterative: 620 objective 17.430829388574267\n",
      "iterative: 630 objective 17.32927100917935\n",
      "iterative: 640 objective 17.228380172822224\n",
      "iterative: 650 objective 17.128152491743766\n",
      "iterative: 660 objective 17.028583607025517\n",
      "iterative: 670 objective 16.929669188400183\n",
      "iterative: 680 objective 16.83140493406361\n",
      "iterative: 690 objective 16.733786570486945\n",
      "iterative: 700 objective 16.636809852231494\n",
      "iterative: 710 objective 16.540470561763623\n",
      "iterative: 720 objective 16.444764509271863\n",
      "iterative: 730 objective 16.349687532484136\n",
      "iterative: 740 objective 16.255235496486993\n",
      "iterative: 750 objective 16.161404293545758\n",
      "iterative: 760 objective 16.068189842925886\n",
      "iterative: 770 objective 15.975588090715664\n",
      "iterative: 780 objective 15.883595009649497\n",
      "iterative: 790 objective 15.79220659893306\n",
      "iterative: 800 objective 15.701418884069335\n",
      "iterative: 810 objective 15.611227916685579\n",
      "iterative: 820 objective 15.521629774361788\n",
      "iterative: 830 objective 15.432620560459883\n",
      "iterative: 840 objective 15.34419640395442\n",
      "iterative: 850 objective 15.256353459264265\n",
      "iterative: 860 objective 15.169087906085267\n",
      "iterative: 870 objective 15.082395949224058\n",
      "iterative: 880 objective 14.99627381843319\n",
      "iterative: 890 objective 14.910717768246926\n",
      "iterative: 900 objective 14.825724077818391\n",
      "iterative: 910 objective 14.741289050758153\n",
      "iterative: 920 objective 14.65740901497276\n",
      "iterative: 930 objective 14.574080322505779\n",
      "iterative: 940 objective 14.491299349378426\n",
      "iterative: 950 objective 14.409062495432597\n",
      "iterative: 960 objective 14.327366184173977\n",
      "iterative: 970 objective 14.246206862616448\n",
      "iterative: 980 objective 14.16558100112762\n",
      "iterative: 990 objective 14.085485093275636\n",
      "iterative: 1000 objective 14.005915655676127\n",
      "iterative: 1010 objective 13.926869227841088\n",
      "iterative: 1020 objective 13.848342372028409\n",
      "iterative: 1030 objective 13.77033167309211\n",
      "iterative: 1040 objective 13.692833738333912\n",
      "iterative: 1050 objective 13.615845197355958\n",
      "iterative: 1060 objective 13.539362701913838\n",
      "iterative: 1070 objective 13.463382925771235\n",
      "iterative: 1080 objective 13.387902564555088\n",
      "iterative: 1090 objective 13.312918335611995\n",
      "iterative: 1100 objective 13.238426977865483\n",
      "iterative: 1110 objective 13.164425251674114\n",
      "iterative: 1120 objective 13.090909938690626\n",
      "iterative: 1130 objective 13.017877841721775\n",
      "iterative: 1140 objective 12.945325784589684\n",
      "iterative: 1150 objective 12.873250611993466\n",
      "iterative: 1160 objective 12.801649189371897\n",
      "iterative: 1170 objective 12.730518402767268\n",
      "iterative: 1180 objective 12.659855158689938\n",
      "iterative: 1190 objective 12.589656383983819\n",
      "iterative: 1200 objective 12.519919025692431\n",
      "iterative: 1210 objective 12.450640050926726\n",
      "iterative: 1220 objective 12.381816446732543\n",
      "iterative: 1230 objective 12.313445219959966\n",
      "iterative: 1240 objective 12.245523397133086\n",
      "iterative: 1250 objective 12.178048024320566\n",
      "iterative: 1260 objective 12.111016167007406\n",
      "iterative: 1270 objective 12.044424909966935\n",
      "iterative: 1280 objective 11.97827135713447\n",
      "iterative: 1290 objective 11.912552631480974\n",
      "iterative: 1300 objective 11.847265874888318\n",
      "iterative: 1310 objective 11.782408248024653\n",
      "iterative: 1320 objective 11.717976930220976\n",
      "iterative: 1330 objective 11.653969119348709\n",
      "iterative: 1340 objective 11.590382031697583\n",
      "iterative: 1350 objective 11.527212901854657\n",
      "iterative: 1360 objective 11.464458982583995\n",
      "iterative: 1370 objective 11.402117544707352\n",
      "iterative: 1380 objective 11.340185876985245\n",
      "iterative: 1390 objective 11.278661285999215\n",
      "iterative: 1400 objective 11.21754109603464\n",
      "iterative: 1410 objective 11.156822648964388\n",
      "iterative: 1420 objective 11.096503304133183\n",
      "iterative: 1430 objective 11.036580438242694\n",
      "iterative: 1440 objective 10.977051445237636\n",
      "iterative: 1450 objective 10.917913736192189\n",
      "iterative: 1460 objective 10.859164739197787\n",
      "iterative: 1470 objective 10.800801899250672\n",
      "iterative: 1480 objective 10.742822678141355\n",
      "iterative: 1490 objective 10.685224554343945\n",
      "iterative: 1500 objective 10.62800502290654\n",
      "iterative: 1510 objective 10.571161595342238\n",
      "iterative: 1520 objective 10.514691799520982\n",
      "iterative: 1530 objective 10.458593179562053\n",
      "iterative: 1540 objective 10.402863295727142\n",
      "iterative: 1550 objective 10.347499724314435\n",
      "iterative: 1560 objective 10.292500057553093\n",
      "iterative: 1570 objective 10.237861903498448\n",
      "iterative: 1580 objective 10.183582885928265\n",
      "iterative: 1590 objective 10.129660644239006\n",
      "iterative: 1600 objective 10.076092833343495\n",
      "iterative: 1610 objective 10.022877123568799\n",
      "iterative: 1620 objective 9.97001120055494\n",
      "iterative: 1630 objective 9.917492765154165\n",
      "iterative: 1640 objective 9.865319533331\n",
      "iterative: 1650 objective 9.813489236063027\n",
      "iterative: 1660 objective 9.761999619242053\n",
      "iterative: 1670 objective 9.71084844357614\n",
      "iterative: 1680 objective 9.660033484492217\n",
      "iterative: 1690 objective 9.60955253203927\n",
      "iterative: 1700 objective 9.559403390792385\n",
      "iterative: 1710 objective 9.509583879757137\n",
      "iterative: 1720 objective 9.460091832274685\n",
      "iterative: 1730 objective 9.410925095927766\n",
      "iterative: 1740 objective 9.362081532446911\n",
      "iterative: 1750 objective 9.313559017617468\n",
      "iterative: 1760 objective 9.265355441187161\n",
      "iterative: 1770 objective 9.217468706774584\n",
      "iterative: 1780 objective 9.169896731777769\n",
      "iterative: 1790 objective 9.122637447283612\n",
      "iterative: 1800 objective 9.07568879797804\n",
      "iterative: 1810 objective 9.029048742056544\n",
      "iterative: 1820 objective 8.98271525113536\n",
      "iterative: 1830 objective 8.936686310163346\n",
      "iterative: 1840 objective 8.8909599173342\n",
      "iterative: 1850 objective 8.845534083999626\n",
      "iterative: 1860 objective 8.800406834582564\n",
      "iterative: 1870 objective 8.755576206491533\n",
      "iterative: 1880 objective 8.711040250035094\n",
      "iterative: 1890 objective 8.66679702833709\n",
      "iterative: 1900 objective 8.622844617252632\n",
      "iterative: 1910 objective 8.57918110528395\n",
      "iterative: 1920 objective 8.535804593497783\n",
      "iterative: 1930 objective 8.492713195442445\n",
      "iterative: 1940 objective 8.449905037065925\n",
      "iterative: 1950 objective 8.407378256634328\n",
      "iterative: 1960 objective 8.365131004650967\n",
      "iterative: 1970 objective 8.323161443775891\n",
      "iterative: 1980 objective 8.281467748745925\n",
      "iterative: 1990 objective 8.240048106295383\n",
      "iterative: 2000 objective 8.1989007150772\n",
      "iterative: 2010 objective 8.15802378558453\n",
      "iterative: 2020 objective 8.117415540072944\n",
      "iterative: 2030 objective 8.077074212483158\n",
      "iterative: 2040 objective 8.03699804836418\n",
      "iterative: 2050 objective 7.997185304796917\n",
      "iterative: 2060 objective 7.957634250318743\n",
      "iterative: 2070 objective 7.918343164847637\n",
      "iterative: 2080 objective 7.879310339607833\n",
      "iterative: 2090 objective 7.8405340770553265\n",
      "iterative: 2100 objective 7.802012690804034\n",
      "iterative: 2110 objective 7.763744505552504\n",
      "iterative: 2120 objective 7.72572785701111\n",
      "iterative: 2130 objective 7.687961091829467\n",
      "iterative: 2140 objective 7.650442567524662\n",
      "iterative: 2150 objective 7.6131706524099645\n",
      "iterative: 2160 objective 7.576143725523566\n",
      "iterative: 2170 objective 7.539360176558346\n",
      "iterative: 2180 objective 7.502818405791663\n",
      "iterative: 2190 objective 7.466516824015888\n",
      "iterative: 2200 objective 7.4304538524692205\n",
      "iterative: 2210 objective 7.394627922767054\n",
      "iterative: 2220 objective 7.3590374768338185\n",
      "iterative: 2230 objective 7.3236809668351555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative: 2240 objective 7.2885568551105715\n",
      "iterative: 2250 objective 7.253663614106631\n",
      "iterative: 2260 objective 7.218999726310535\n",
      "iterative: 2270 objective 7.184563684184077\n",
      "iterative: 2280 objective 7.150353990098116\n",
      "iterative: 2290 objective 7.116369156267389\n",
      "iterative: 2300 objective 7.0826077046858975\n",
      "iterative: 2310 objective 7.049068167062463\n",
      "iterative: 2320 objective 7.015749084757107\n",
      "iterative: 2330 objective 6.98264900871738\n",
      "iterative: 2340 objective 6.949766499415581\n",
      "iterative: 2350 objective 6.9171001267859245\n",
      "iterative: 2360 objective 6.884648470162439\n",
      "iterative: 2370 objective 6.852410118217209\n",
      "iterative: 2380 objective 6.820383668899081\n",
      "iterative: 2390 objective 6.7885677293723745\n",
      "iterative: 2400 objective 6.756960915956663\n",
      "iterative: 2410 objective 6.7255618540664\n",
      "iterative: 2420 objective 6.6943691781512635\n",
      "iterative: 2430 objective 6.663381531636494\n",
      "iterative: 2440 objective 6.632597566864363\n",
      "iterative: 2450 objective 6.602015945035064\n",
      "iterative: 2460 objective 6.571635336148868\n",
      "iterative: 2470 objective 6.541454418948041\n",
      "iterative: 2480 objective 6.511471880859522\n",
      "iterative: 2490 objective 6.48168641793777\n",
      "iterative: 2500 objective 6.452096734808098\n",
      "iterative: 2510 objective 6.4227015446102875\n",
      "iterative: 2520 objective 6.393499568942648\n",
      "iterative: 2530 objective 6.36448953780649\n",
      "iterative: 2540 objective 6.3356701895506875\n",
      "iterative: 2550 objective 6.307040270817086\n",
      "iterative: 2560 objective 6.278598536485756\n",
      "iterative: 2570 objective 6.25034374962098\n",
      "iterative: 2580 objective 6.222274681417405\n",
      "iterative: 2590 objective 6.194390111146639\n",
      "iterative: 2600 objective 6.166688826104093\n",
      "iterative: 2610 objective 6.139169621556365\n",
      "iterative: 2620 objective 6.111831300688573\n",
      "iterative: 2630 objective 6.08467267455277\n",
      "iterative: 2640 objective 6.057692562015752\n",
      "iterative: 2650 objective 6.030889789707972\n",
      "iterative: 2660 objective 6.00426319197242\n",
      "iterative: 2670 objective 5.977811610814\n",
      "iterative: 2680 objective 5.951533895849031\n",
      "iterative: 2690 objective 5.925428904255378\n",
      "iterative: 2700 objective 5.899495500722622\n",
      "iterative: 2710 objective 5.873732557402816\n",
      "iterative: 2720 objective 5.848138953861244\n",
      "iterative: 2730 objective 5.822713577027929\n",
      "iterative: 2740 objective 5.797455321148902\n",
      "iterative: 2750 objective 5.772363087738539\n",
      "iterative: 2760 objective 5.7474357855314455\n",
      "iterative: 2770 objective 5.722672330435143\n",
      "iterative: 2780 objective 5.698071645482843\n",
      "iterative: 2790 objective 5.673632660786751\n",
      "iterative: 2800 objective 5.649354313491353\n",
      "iterative: 2810 objective 5.625235547727324\n",
      "iterative: 2820 objective 5.601275314565532\n",
      "iterative: 2830 objective 5.577472571971464\n",
      "iterative: 2840 objective 5.553826284759937\n",
      "iterative: 2850 objective 5.53033542455005\n",
      "iterative: 2860 objective 5.5069989697203585\n",
      "iterative: 2870 objective 5.483815905364621\n",
      "iterative: 2880 objective 5.460785223247537\n",
      "iterative: 2890 objective 5.437905921760953\n",
      "iterative: 2900 objective 5.415177005880211\n",
      "iterative: 2910 objective 5.392597487121063\n",
      "iterative: 2920 objective 5.3701663834964615\n",
      "iterative: 2930 objective 5.347882719474042\n",
      "iterative: 2940 objective 5.325745525933552\n",
      "iterative: 2950 objective 5.303753840124787\n",
      "iterative: 2960 objective 5.281906705625718\n",
      "iterative: 2970 objective 5.260203172300851\n",
      "iterative: 2980 objective 5.238642296259939\n",
      "iterative: 2990 objective 5.217223139816886\n",
      "iterative: 3000 objective 5.1959447714490805\n",
      "iterative: 3010 objective 5.174806265756752\n",
      "iterative: 3020 objective 5.153806703422816\n",
      "iterative: 3030 objective 5.132945171172795\n",
      "iterative: 3040 objective 5.112220761735249\n",
      "iterative: 3050 objective 5.0916325738021735\n",
      "iterative: 3060 objective 5.071179711989867\n",
      "iterative: 3070 objective 5.050861286799986\n",
      "iterative: 3080 objective 5.030676414580861\n",
      "iterative: 3090 objective 5.010624217489093\n",
      "iterative: 3100 objective 4.990703823451313\n",
      "iterative: 3110 objective 4.970914366126253\n",
      "iterative: 3120 objective 4.951254984867183\n",
      "iterative: 3130 objective 4.931724824684352\n",
      "iterative: 3140 objective 4.912323036207875\n",
      "iterative: 3150 objective 4.893048775650776\n",
      "iterative: 3160 objective 4.873901204772253\n",
      "iterative: 3170 objective 4.854879490841333\n",
      "iterative: 3180 objective 4.8359828066004615\n",
      "iterative: 3190 objective 4.817210330229755\n",
      "iterative: 3200 objective 4.798561245311122\n",
      "iterative: 3210 objective 4.780034740792782\n",
      "iterative: 3220 objective 4.761630010953976\n",
      "iterative: 3230 objective 4.74334625536997\n",
      "iterative: 3240 objective 4.725182678877224\n",
      "iterative: 3250 objective 4.707138491538806\n",
      "iterative: 3260 objective 4.689212908610002\n",
      "iterative: 3270 objective 4.671405150504253\n",
      "iterative: 3280 objective 4.6537144427592345\n",
      "iterative: 3290 objective 4.6361400160031065\n",
      "iterative: 3300 objective 4.618681105921205\n",
      "iterative: 3310 objective 4.601336953222601\n",
      "iterative: 3320 objective 4.5841068036072405\n",
      "iterative: 3330 objective 4.566989907733086\n",
      "iterative: 3340 objective 4.549985521183471\n",
      "iterative: 3350 objective 4.533092904434839\n",
      "iterative: 3360 objective 4.516311322824545\n",
      "iterative: 3370 objective 4.499640046518735\n",
      "iterative: 3380 objective 4.483078350480921\n",
      "iterative: 3390 objective 4.466625514440166\n",
      "iterative: 3400 objective 4.4502808228599475\n",
      "iterative: 3410 objective 4.434043564906923\n",
      "iterative: 3420 objective 4.417913034420039\n",
      "iterative: 3430 objective 4.401888529879876\n",
      "iterative: 3440 objective 4.385969354378074\n",
      "iterative: 3450 objective 4.370154815587066\n",
      "iterative: 3460 objective 4.354444225729942\n",
      "iterative: 3470 objective 4.338836901550561\n",
      "iterative: 3480 objective 4.323332164283844\n",
      "iterative: 3490 objective 4.3079293396261304\n",
      "iterative: 3500 objective 4.2926277577060805\n",
      "iterative: 3510 objective 4.277426753055418\n",
      "iterative: 3520 objective 4.262325664579894\n",
      "iterative: 3530 objective 4.247323835530664\n",
      "iterative: 3540 objective 4.232420613475694\n",
      "iterative: 3550 objective 4.217615350271435\n",
      "iterative: 3560 objective 4.202907402034509\n",
      "iterative: 3570 objective 4.188296129113836\n",
      "iterative: 3580 objective 4.173780896062721\n",
      "iterative: 3590 objective 4.159361071611356\n",
      "iterative: 3600 objective 4.14503602863909\n",
      "iterative: 3610 objective 4.130805144147482\n",
      "iterative: 3620 objective 4.116667799233052\n",
      "iterative: 3630 objective 4.102623379060262\n",
      "iterative: 3640 objective 4.088671272835004\n",
      "iterative: 3650 objective 4.074810873777876\n",
      "iterative: 3660 objective 4.061041579097819\n",
      "iterative: 3670 objective 4.047362789965949\n",
      "iterative: 3680 objective 4.03377391148944\n",
      "iterative: 3690 objective 4.020274352685745\n",
      "iterative: 3700 objective 4.006863526456825\n",
      "iterative: 3710 objective 3.993540849563632\n",
      "iterative: 3720 objective 3.980305742600709\n",
      "iterative: 3730 objective 3.967157629971099\n",
      "iterative: 3740 objective 3.954095939861203\n",
      "iterative: 3750 objective 3.9411201042159836\n",
      "iterative: 3760 objective 3.928229558714197\n",
      "iterative: 3770 objective 3.9154237427438594\n",
      "iterative: 3780 objective 3.902702099377947\n",
      "iterative: 3790 objective 3.8900640753500983\n",
      "iterative: 3800 objective 3.877509121030535\n",
      "iterative: 3810 objective 3.865036690402224\n",
      "iterative: 3820 objective 3.8526462410371085\n",
      "iterative: 3830 objective 3.8403372340725066\n",
      "iterative: 3840 objective 3.8281091341876525\n",
      "iterative: 3850 objective 3.815961409580476\n",
      "iterative: 3860 objective 3.803893531944399\n",
      "iterative: 3870 objective 3.7919049764454713\n",
      "iterative: 3880 objective 3.779995221699406\n",
      "iterative: 3890 objective 3.768163749749001\n",
      "iterative: 3900 objective 3.7564100460415917\n",
      "iterative: 3910 objective 3.744733599406667\n",
      "iterative: 3920 objective 3.733133902033634\n",
      "iterative: 3930 objective 3.721610449449725\n",
      "iterative: 3940 objective 3.710162740498127\n",
      "iterative: 3950 objective 3.6987902773160255\n",
      "iterative: 3960 objective 3.6874925653131743\n",
      "iterative: 3970 objective 3.6762691131501666\n",
      "iterative: 3980 objective 3.665119432717239\n",
      "iterative: 3990 objective 3.6540430391129415\n",
      "iterative: 4000 objective 3.6430394506230828\n",
      "iterative: 4010 objective 3.632108188699763\n",
      "iterative: 4020 objective 3.6212487779406444\n",
      "iterative: 4030 objective 3.6104607460681026\n",
      "iterative: 4040 objective 3.5997436239089273\n",
      "iterative: 4050 objective 3.589096945373627\n",
      "iterative: 4060 objective 3.578520247436438\n",
      "iterative: 4070 objective 3.568013070115034\n",
      "iterative: 4080 objective 3.557574956450451\n",
      "iterative: 4090 objective 3.54720545248749\n",
      "iterative: 4100 objective 3.536904107254655\n",
      "iterative: 4110 objective 3.526670472744792\n",
      "iterative: 4120 objective 3.5165041038954272\n",
      "iterative: 4130 objective 3.5064045585694714\n",
      "iterative: 4140 objective 3.4963713975360746\n",
      "iterative: 4150 objective 3.4864041844513665\n",
      "iterative: 4160 objective 3.4765024858396165\n",
      "iterative: 4170 objective 3.466665871074333\n",
      "iterative: 4180 objective 3.4568939123595017\n",
      "iterative: 4190 objective 3.447186184711006\n",
      "iterative: 4200 objective 3.4375422659382084\n",
      "iterative: 4210 objective 3.4279617366254587\n",
      "iterative: 4220 objective 3.418444180113954\n",
      "iterative: 4230 objective 3.4089891824835967\n",
      "iterative: 4240 objective 3.3995963325349656\n",
      "iterative: 4250 objective 3.390265221771496\n",
      "iterative: 4260 objective 3.3809954443816213\n",
      "iterative: 4270 objective 3.3717865972211754\n",
      "iterative: 4280 objective 3.362638279795889\n",
      "iterative: 4290 objective 3.353550094243959\n",
      "iterative: 4300 objective 3.3445216453186295\n",
      "iterative: 4310 objective 3.3355525403712223\n",
      "iterative: 4320 objective 3.326642389333832\n",
      "iterative: 4330 objective 3.3177908047025753\n",
      "iterative: 4340 objective 3.308997401520498\n",
      "iterative: 4350 objective 3.300261797361121\n",
      "iterative: 4360 objective 3.2915836123115465\n",
      "iterative: 4370 objective 3.2829624689560535\n",
      "iterative: 4380 objective 3.2743979923597135\n",
      "iterative: 4390 objective 3.265889810052031\n",
      "iterative: 4400 objective 3.2574375520107246\n",
      "iterative: 4410 objective 3.2490408506457262\n",
      "iterative: 4420 objective 3.240699340783081\n",
      "iterative: 4430 objective 3.232412659649187\n",
      "iterative: 4440 objective 3.2241804468548665\n",
      "iterative: 4450 objective 3.2160023443798407\n",
      "iterative: 4460 objective 3.207877996557102\n",
      "iterative: 4470 objective 3.199807050057392\n",
      "iterative: 4480 objective 3.191789153873917\n",
      "iterative: 4490 objective 3.1838239593070945\n",
      "iterative: 4500 objective 3.175911119949228\n",
      "iterative: 4510 objective 3.168050291669674\n",
      "iterative: 4520 objective 3.160241132599693\n",
      "iterative: 4530 objective 3.1524833031176556\n",
      "iterative: 4540 objective 3.144776465834327\n",
      "iterative: 4550 objective 3.137120285578039\n",
      "iterative: 4560 objective 3.129514429380233\n",
      "iterative: 4570 objective 3.12195856646099\n",
      "iterative: 4580 objective 3.1144523682145966\n",
      "iterative: 4590 objective 3.1069955081952294\n",
      "iterative: 4600 objective 3.099587662102823\n",
      "iterative: 4610 objective 3.092228507768913\n",
      "iterative: 4620 objective 3.0849177251427014\n",
      "iterative: 4630 objective 3.0776549962769857\n",
      "iterative: 4640 objective 3.0704400053145218\n",
      "iterative: 4650 objective 3.0632724384741765\n",
      "iterative: 4660 objective 3.05615198403726\n",
      "iterative: 4670 objective 3.04907833233405\n",
      "iterative: 4680 objective 3.042051175730224\n",
      "iterative: 4690 objective 3.0350702086136483\n",
      "iterative: 4700 objective 3.028135127380847\n",
      "iterative: 4710 objective 3.0212456304240183\n",
      "iterative: 4720 objective 3.014401418117793\n",
      "iterative: 4730 objective 3.0076021928062704\n",
      "iterative: 4740 objective 3.000847658789993\n",
      "iterative: 4750 objective 2.9941375223131677\n",
      "iterative: 4760 objective 2.9874714915508593\n",
      "iterative: 4770 objective 2.9808492765962695\n",
      "iterative: 4780 objective 2.9742705894482078\n",
      "iterative: 4790 objective 2.967735143998471\n",
      "iterative: 4800 objective 2.961242656019404\n",
      "iterative: 4810 objective 2.954792843151722\n",
      "iterative: 4820 objective 2.9483854248919616\n",
      "iterative: 4830 objective 2.9420201225804066\n",
      "iterative: 4840 objective 2.93569665938906\n",
      "iterative: 4850 objective 2.9294147603094265\n",
      "iterative: 4860 objective 2.9231741521407164\n",
      "iterative: 4870 objective 2.916974563477811\n",
      "iterative: 4880 objective 2.9108157246995945\n",
      "iterative: 4890 objective 2.904697367957128\n",
      "iterative: 4900 objective 2.8986192271621096\n",
      "iterative: 4910 objective 2.892581037975147\n",
      "iterative: 4920 objective 2.88658253779443\n",
      "iterative: 4930 objective 2.8806234657441814\n",
      "iterative: 4940 objective 2.8747035626633672\n",
      "iterative: 4950 objective 2.868822571094424\n",
      "iterative: 4960 objective 2.862980235272076\n",
      "iterative: 4970 objective 2.8571763011121383\n",
      "iterative: 4980 objective 2.851410516200613\n",
      "iterative: 4990 objective 2.845682629782485\n",
      "iterative: 5000 objective 2.8399923927510997\n",
      "iterative: 5010 objective 2.8343395576370742\n",
      "iterative: 5020 objective 2.828723878597652\n",
      "iterative: 5030 objective 2.8231451114060855\n",
      "iterative: 5040 objective 2.8176030134408085\n",
      "iterative: 5050 objective 2.812097343675097\n",
      "iterative: 5060 objective 2.8066278626664825\n",
      "iterative: 5070 objective 2.801194332546294\n",
      "iterative: 5080 objective 2.7957965170094194\n",
      "iterative: 5090 objective 2.79043418130397\n",
      "iterative: 5100 objective 2.7851070922210606\n",
      "iterative: 5110 objective 2.779815018084745\n",
      "iterative: 5120 objective 2.7745577287418235\n",
      "iterative: 5130 objective 2.769334995551954\n",
      "iterative: 5140 objective 2.764146591377534\n",
      "iterative: 5150 objective 2.7589922905740574\n",
      "iterative: 5160 objective 2.7538718689801214\n",
      "iterative: 5170 objective 2.7487851039077613\n",
      "iterative: 5180 objective 2.7437317741326854\n",
      "iterative: 5190 objective 2.7387116598847645\n",
      "iterative: 5200 objective 2.7337245428384027\n",
      "iterative: 5210 objective 2.728770206103068\n",
      "iterative: 5220 objective 2.723848434213835\n",
      "iterative: 5230 objective 2.7189590131220083\n",
      "iterative: 5240 objective 2.7141017301858854\n",
      "iterative: 5250 objective 2.709276374161426\n",
      "iterative: 5260 objective 2.704482735193069\n",
      "iterative: 5270 objective 2.6997206048046793\n",
      "iterative: 5280 objective 2.694989775890429\n",
      "iterative: 5290 objective 2.690290042705806\n",
      "iterative: 5300 objective 2.6856212008585922\n",
      "iterative: 5310 objective 2.680983047300122\n",
      "iterative: 5320 objective 2.676375380316298\n",
      "iterative: 5330 objective 2.6717979995189487\n",
      "iterative: 5340 objective 2.6672507058370285\n",
      "iterative: 5350 objective 2.6627333015079797\n",
      "iterative: 5360 objective 2.658245590069111\n",
      "iterative: 5370 objective 2.6537873763491184\n",
      "iterative: 5380 objective 2.6493584664596304\n",
      "iterative: 5390 objective 2.644958667786515\n",
      "iterative: 5400 objective 2.640587788981865\n",
      "iterative: 5410 objective 2.636245639955399\n",
      "iterative: 5420 objective 2.6319320318663966\n",
      "iterative: 5430 objective 2.627646777115266\n",
      "iterative: 5440 objective 2.623389689335605\n",
      "iterative: 5450 objective 2.619160583385969\n",
      "iterative: 5460 objective 2.614959275341824\n",
      "iterative: 5470 objective 2.6107855824875776\n",
      "iterative: 5480 objective 2.6066393233086727\n",
      "iterative: 5490 objective 2.602520317483592\n",
      "iterative: 5500 objective 2.598428385876087\n",
      "iterative: 5510 objective 2.5943633505274057\n",
      "iterative: 5520 objective 2.5903250346484343\n",
      "iterative: 5530 objective 2.586313262612191\n",
      "iterative: 5540 objective 2.582327859946065\n",
      "iterative: 5550 objective 2.5783686533241905\n",
      "iterative: 5560 objective 2.5744354705600334\n",
      "iterative: 5570 objective 2.5705281405988263\n",
      "iterative: 5580 objective 2.566646493510158\n",
      "iterative: 5590 objective 2.5627903604805375\n",
      "iterative: 5600 objective 2.5589595738060926\n",
      "iterative: 5610 objective 2.555153966885323\n",
      "iterative: 5620 objective 2.551373374211706\n",
      "iterative: 5630 objective 2.547617631366652\n",
      "iterative: 5640 objective 2.5438865750123054\n",
      "iterative: 5650 objective 2.5401800428844106\n",
      "iterative: 5660 objective 2.5364978737852404\n",
      "iterative: 5670 objective 2.532839907576687\n",
      "iterative: 5680 objective 2.529205985173145\n",
      "iterative: 5690 objective 2.525595948534794\n",
      "iterative: 5700 objective 2.522009640660509\n",
      "iterative: 5710 objective 2.518446905581164\n",
      "iterative: 5720 objective 2.5149075883528407\n",
      "iterative: 5730 objective 2.5113915350500453\n",
      "iterative: 5740 objective 2.5078985927590236\n",
      "iterative: 5750 objective 2.504428609571149\n",
      "iterative: 5760 objective 2.5009814345762735\n",
      "iterative: 5770 objective 2.4975569178561994\n",
      "iterative: 5780 objective 2.494154910478103\n",
      "iterative: 5790 objective 2.490775264488151\n",
      "iterative: 5800 objective 2.4874178329049697\n",
      "iterative: 5810 objective 2.484082469713318\n",
      "iterative: 5820 objective 2.4807690298576675\n",
      "iterative: 5830 objective 2.477477369235997\n",
      "iterative: 5840 objective 2.4742073446934305\n",
      "iterative: 5850 objective 2.4709588140160688\n",
      "iterative: 5860 objective 2.4677316359247823\n",
      "iterative: 5870 objective 2.4645256700690323\n",
      "iterative: 5880 objective 2.461340777020867\n",
      "iterative: 5890 objective 2.458176818268751\n",
      "iterative: 5900 objective 2.455033656211578\n",
      "iterative: 5910 objective 2.4519111541527687\n",
      "iterative: 5920 objective 2.4488091762941036\n",
      "iterative: 5930 objective 2.445727587730133\n",
      "iterative: 5940 objective 2.4426662544420075\n",
      "iterative: 5950 objective 2.4396250432918736\n",
      "iterative: 5960 objective 2.436603822016944\n",
      "iterative: 5970 objective 2.4336024592237937\n",
      "iterative: 5980 objective 2.430620824382669\n",
      "iterative: 5990 objective 2.4276587878217764\n",
      "iterative: 6000 objective 2.42471622072166\n",
      "iterative: 6010 objective 2.421792995109596\n",
      "iterative: 6020 objective 2.418888983854016\n",
      "iterative: 6030 objective 2.4160040606589677\n",
      "iterative: 6040 objective 2.413138100058698\n",
      "iterative: 6050 objective 2.410290977412088\n",
      "iterative: 6060 objective 2.4074625688973037\n",
      "iterative: 6070 objective 2.4046527515063985\n",
      "iterative: 6080 objective 2.401861403039955\n",
      "iterative: 6090 objective 2.3990884021017442\n",
      "iterative: 6100 objective 2.3963336280935383\n",
      "iterative: 6110 objective 2.3935969612097203\n",
      "iterative: 6120 objective 2.3908782824322072\n",
      "iterative: 6130 objective 2.3881774735252286\n",
      "iterative: 6140 objective 2.385494417030136\n",
      "iterative: 6150 objective 2.382828996260366\n",
      "iterative: 6160 objective 2.3801810952963223\n",
      "iterative: 6170 objective 2.3775505989803762\n",
      "iterative: 6180 objective 2.3749373929117836\n",
      "iterative: 6190 objective 2.3723413634417923\n",
      "iterative: 6200 objective 2.3697623976686617\n",
      "iterative: 6210 objective 2.367200383432742\n",
      "iterative: 6220 objective 2.3646552093115827\n",
      "iterative: 6230 objective 2.3621267646151596\n",
      "iterative: 6240 objective 2.359614939381001\n",
      "iterative: 6250 objective 2.3571196243694126\n",
      "iterative: 6260 objective 2.354640711058721\n",
      "iterative: 6270 objective 2.3521780916405923\n",
      "iterative: 6280 objective 2.3497316590152852\n",
      "iterative: 6290 objective 2.3473013067870467\n",
      "iterative: 6300 objective 2.3448869292594643\n",
      "iterative: 6310 objective 2.342488421430849\n",
      "iterative: 6320 objective 2.3401056789897074\n",
      "iterative: 6330 objective 2.3377385983101746\n",
      "iterative: 6340 objective 2.33538707644752\n",
      "iterative: 6350 objective 2.3330510111336706\n",
      "iterative: 6360 objective 2.330730300772764\n",
      "iterative: 6370 objective 2.328424844436718\n",
      "iterative: 6380 objective 2.326134541860855\n",
      "iterative: 6390 objective 2.3238592934395395\n",
      "iterative: 6400 objective 2.3215990002218327\n",
      "iterative: 6410 objective 2.3193535639072227\n",
      "iterative: 6420 objective 2.3171228868413136\n",
      "iterative: 6430 objective 2.3149068720115755\n",
      "iterative: 6440 objective 2.3127054230431754\n",
      "iterative: 6450 objective 2.310518444194723\n",
      "iterative: 6460 objective 2.308345840354152\n",
      "iterative: 6470 objective 2.3061875170345605\n",
      "iterative: 6480 objective 2.3040433803701106\n",
      "iterative: 6490 objective 2.301913337111948\n",
      "iterative: 6500 objective 2.299797294624138\n",
      "iterative: 6510 objective 2.297695160879643\n",
      "iterative: 6520 objective 2.2956068444563176\n",
      "iterative: 6530 objective 2.2935322545329373\n",
      "iterative: 6540 objective 2.291471300885243\n",
      "iterative: 6550 objective 2.2894238938820126\n",
      "iterative: 6560 objective 2.2873899444811903\n",
      "iterative: 6570 objective 2.2853693642259767\n",
      "iterative: 6580 objective 2.2833620652409983\n",
      "iterative: 6590 objective 2.2813679602284944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative: 6600 objective 2.2793869624645176\n",
      "iterative: 6610 objective 2.277418985795114\n",
      "iterative: 6620 objective 2.2754639446326963\n",
      "iterative: 6630 objective 2.2735217539521773\n",
      "iterative: 6640 objective 2.2715923292873628\n",
      "iterative: 6650 objective 2.2696755867272684\n",
      "iterative: 6660 objective 2.2677714429124407\n",
      "iterative: 6670 objective 2.265879815031338\n",
      "iterative: 6680 objective 2.2640006208167662\n",
      "iterative: 6690 objective 2.262133778542253\n",
      "iterative: 6700 objective 2.2602792070185025\n",
      "iterative: 6710 objective 2.2584368255899196\n",
      "iterative: 6720 objective 2.256606554131021\n",
      "iterative: 6730 objective 2.254788313042985\n",
      "iterative: 6740 objective 2.2529820232502416\n",
      "iterative: 6750 objective 2.2511876061969116\n",
      "iterative: 6760 objective 2.2494049838435206\n",
      "iterative: 6770 objective 2.2476340786635305\n",
      "iterative: 6780 objective 2.245874813639969\n",
      "iterative: 6790 objective 2.2441271122621176\n",
      "iterative: 6800 objective 2.242390898522151\n",
      "iterative: 6810 objective 2.2406660969118497\n",
      "iterative: 6820 objective 2.23895263241932\n",
      "iterative: 6830 objective 2.23725043052568\n",
      "iterative: 6840 objective 2.23555941720191\n",
      "iterative: 6850 objective 2.2338795189055527\n",
      "iterative: 6860 objective 2.2322106625775513\n",
      "iterative: 6870 objective 2.2305527756390804\n",
      "iterative: 6880 objective 2.2289057859883408\n",
      "iterative: 6890 objective 2.227269621997508\n",
      "iterative: 6900 objective 2.2256442125095144\n",
      "iterative: 6910 objective 2.2240294868350516\n",
      "iterative: 6920 objective 2.2224253747494336\n",
      "iterative: 6930 objective 2.2208318064895454\n",
      "iterative: 6940 objective 2.219248712750858\n",
      "iterative: 6950 objective 2.2176760246843417\n",
      "iterative: 6960 objective 2.2161136738935454\n",
      "iterative: 6970 objective 2.214561592431579\n",
      "iterative: 6980 objective 2.21301971279816\n",
      "iterative: 6990 objective 2.211487967936684\n",
      "iterative: 7000 objective 2.209966291231316\n",
      "iterative: 7010 objective 2.208454616504092\n",
      "iterative: 7020 objective 2.2069528780120082\n",
      "iterative: 7030 objective 2.2054610104442403\n",
      "iterative: 7040 objective 2.2039789489191906\n",
      "iterative: 7050 objective 2.202506628981794\n",
      "iterative: 7060 objective 2.2010439866005798\n",
      "iterative: 7070 objective 2.199590958164999\n",
      "iterative: 7080 objective 2.198147480482605\n",
      "iterative: 7090 objective 2.1967134907763226\n",
      "iterative: 7100 objective 2.1952889266816937\n",
      "iterative: 7110 objective 2.1938737262442056\n",
      "iterative: 7120 objective 2.1924678279165284\n",
      "iterative: 7130 objective 2.1910711705559356\n",
      "iterative: 7140 objective 2.1896836934215687\n",
      "iterative: 7150 objective 2.1883053361718043\n",
      "iterative: 7160 objective 2.1869360388616674\n",
      "iterative: 7170 objective 2.185575741940185\n",
      "iterative: 7180 objective 2.184224386247827\n",
      "iterative: 7190 objective 2.182881913013905\n",
      "iterative: 7200 objective 2.1815482638540376\n",
      "iterative: 7210 objective 2.180223380767609\n",
      "iterative: 7220 objective 2.1789072061352246\n",
      "iterative: 7230 objective 2.1775996827162363\n",
      "iterative: 7240 objective 2.176300753646233\n",
      "iterative: 7250 objective 2.175010362434577\n",
      "iterative: 7260 objective 2.173728452961947\n",
      "iterative: 7270 objective 2.1724549694778763\n",
      "iterative: 7280 objective 2.171189856598368\n",
      "iterative: 7290 objective 2.169933059303446\n",
      "iterative: 7300 objective 2.1686845229348\n",
      "iterative: 7310 objective 2.1674441931933677\n",
      "iterative: 7320 objective 2.1662120161370026\n",
      "iterative: 7330 objective 2.1649879381781343\n",
      "iterative: 7340 objective 2.163771906081387\n",
      "iterative: 7350 objective 2.162563866961348\n",
      "iterative: 7360 objective 2.1613637682801854\n",
      "iterative: 7370 objective 2.160171557845394\n",
      "iterative: 7380 objective 2.1589871838075703\n",
      "iterative: 7390 objective 2.1578105946580832\n",
      "iterative: 7400 objective 2.1566417392268735\n",
      "iterative: 7410 objective 2.1554805666802244\n",
      "iterative: 7420 objective 2.1543270265185663\n",
      "iterative: 7430 objective 2.1531810685742183\n",
      "iterative: 7440 objective 2.1520426430093034\n",
      "iterative: 7450 objective 2.1509117003134897\n",
      "iterative: 7460 objective 2.1497881913019006\n",
      "iterative: 7470 objective 2.1486720671129387\n",
      "iterative: 7480 objective 2.1475632792061834\n",
      "iterative: 7490 objective 2.146461779360274\n",
      "iterative: 7500 objective 2.1453675196707915\n",
      "iterative: 7510 objective 2.1442804525482\n",
      "iterative: 7520 objective 2.1432005307157707\n",
      "iterative: 7530 objective 2.1421277072075284\n",
      "iterative: 7540 objective 2.141061935366191\n",
      "iterative: 7550 objective 2.1400031688411802\n",
      "iterative: 7560 objective 2.1389513615865536\n",
      "iterative: 7570 objective 2.1379064678590476\n",
      "iterative: 7580 objective 2.1368684422160547\n",
      "iterative: 7590 objective 2.1358372395136733\n",
      "iterative: 7600 objective 2.134812814904726\n",
      "iterative: 7610 objective 2.133795123836817\n",
      "iterative: 7620 objective 2.132784122050395\n",
      "iterative: 7630 objective 2.1317797655768187\n",
      "iterative: 7640 objective 2.1307820107364734\n",
      "final m= -0.015084249153162945\n",
      "final c= 33.01051337740988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bbef5c0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 3 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "m=-0.4\n",
    "c=80\n",
    "diff=1\n",
    "ite=0\n",
    "f = m*x + c\n",
    "obj = ((y-f)**2).sum()\n",
    "while(diff>=0.0001):\n",
    "    for i in np.arange(10):\n",
    "        old_obj=obj\n",
    "        m = ((y - c)*x).sum()/(x*x).sum()\n",
    "        c = (y-m*x).sum()/y.shape[0]\n",
    "        f = m*x + c\n",
    "        obj = ((y-f)**2).sum()\n",
    "        ite+=1\n",
    "    diff=old_obj-obj\n",
    "    print(\"iterative:\",ite,\"objective\",obj)\n",
    "print(\"final m=\",m)\n",
    "print(\"final c=\",c)\n",
    "\n",
    "f_test = m*x_test + c\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer Here\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Input Solution with Linear Algebra\n",
    "\n",
    "You've now seen how slow it can be to perform a coordinate ascent on a system. Another approach to solving the system (which is not always possible, particularly in *non-linear* systems) is to go directly to the minimum. To do this we need to introduce *linear algebra*. We will represent all our errors and functions in the form of linear algebra. \n",
    "\n",
    "As we mentioned above, linear algebra is just a shorthand for performing lots of multiplications and additions simultaneously. What does it have to do with our system then? Well the first thing to note is that the linear function we were trying to fit has the following form:\n",
    "$$\n",
    "f(x) = mx + c\n",
    "$$\n",
    "the classical form for a straight line. From a linear algebraic perspective we are looking for multiplications and additions. We are also looking to separate our parameters from our data. The data is the *givens* remember, in French the word is donn√©es literally translated means *givens* that's great, because we don't need to change the data, what we need to change are the parameters (or variables) of the model. In this function the data comes in through $x$, and the parameters are $m$ and $c$. \n",
    "\n",
    "What we'd like to create is a vector of parameters and a vector of data. Then we could represent the system with vectors that represent the data, and vectors that represent the parameters. \n",
    "\n",
    "We look to turn the multiplications and additions into a linear algebraic form, we have one multiplication ($m\\times c$) and one addition ($mx + c$). But we can turn this into a inner product by writing it in the following way,\n",
    "$$\n",
    "f(x) = m \\times x + c \\times 1,\n",
    "$$\n",
    "in other words we've extracted the unit value, from the offset, $c$. We can think of this unit value like an extra item of data, because it is always given to us, and it is always set to 1 (unlike regular data, which is likely to vary!). We can therefore write each input data location, $\\mathbf{x}$, as a vector\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we choose to also turn our parameters into a vector. The parameter vector will be defined to contain \n",
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
    "$$\n",
    "because if we now take the inner product between these to vectors we recover\n",
    "$$\n",
    "\\mathbf{x}\\cdot\\mathbf{w} = 1 \\times c + x \\times m = mx + c\n",
    "$$\n",
    "In `numpy` we can define this vector as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.30105134e+01]\n",
      " [-1.50842492e-02]]\n"
     ]
    }
   ],
   "source": [
    "# define the vector w\n",
    "w = np.zeros(shape=(2, 1))\n",
    "w[0] = c\n",
    "w[1] = m\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the equivalence between original operation and an operation in vector space. Whilst the notation here isn't a lot shorter, the beauty is that we will be able to add as many features as we like and still keep the same representation. In general, we are now moving to a system where each of our predictions is given by an inner product. When we want to represent a linear product in linear algebra, we tend to do it with the transpose operation, so since we have $\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write\n",
    "$$\n",
    "f(\\mathbf{x}_i) = \\mathbf{x}_i^\\top\\mathbf{w}.\n",
    "$$\n",
    "Where we've assumed that each data point, $\\mathbf{x}_i$, is now written by appending a 1 onto the original vector\n",
    "$$\n",
    "\\mathbf{x}_i = \n",
    "\\begin{bmatrix} \n",
    "1 \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Design Matrix\n",
    "\n",
    "We can do this for the entire data set to form a [*design matrix*](http://en.wikipedia.org/wiki/Design_matrix) $\\mathbf{X}$,\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "\\mathbf{x}_1^\\top \\\\\\ \n",
    "\\mathbf{x}_2^\\top \\\\\\ \n",
    "\\vdots \\\\\\\n",
    "\\mathbf{x}_n^\\top\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\\\n",
    "1 & x_2 \\\\\\\n",
    "\\vdots & \\vdots \\\\\\\n",
    "1 & x_n \n",
    "\\end{bmatrix},$$\n",
    "\n",
    "which in `numpy` can be done with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1 1896]\n",
      " [   1 1900]\n",
      " [   1 1904]\n",
      " [   1 1908]\n",
      " [   1 1912]\n",
      " [   1 1920]\n",
      " [   1 1924]\n",
      " [   1 1928]\n",
      " [   1 1932]\n",
      " [   1 1936]\n",
      " [   1 1948]\n",
      " [   1 1952]\n",
      " [   1 1956]\n",
      " [   1 1960]\n",
      " [   1 1964]\n",
      " [   1 1968]\n",
      " [   1 1972]\n",
      " [   1 1976]\n",
      " [   1 1980]\n",
      " [   1 1984]\n",
      " [   1 1988]\n",
      " [   1 1992]\n",
      " [   1 1996]\n",
      " [   1 2000]\n",
      " [   1 2004]\n",
      " [   1 2008]\n",
      " [   1 2012]]\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Objective with Linear Algebra\n",
    "\n",
    "When we think of the objective function, we can think of it as the errors where the error is defined in a similar way to what it was in Legendre's day $y_i - f(\\mathbf{x}_i)$, in statistics these errors are also sometimes called [*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics). So we can think as the objective and the prediction function as two separate parts, first we have,\n",
    "$$\n",
    "E(\\mathbf{w}) = \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2,\n",
    "$$\n",
    "where we've made the function $f(\\cdot)$'s dependence on the parameters $\\mathbf{w}$ explicit in this equation. Then we have the definition of the function itself,\n",
    "$$\n",
    "f(\\mathbf{x}_i; \\mathbf{w}) = \\mathbf{x}_i^\\top \\mathbf{w}.\n",
    "$$\n",
    "Let's look again at these two equations and see if we can identify any inner products. The first equation is a sum of squares, which is promising. Any sum of squares can be represented by an inner product,\n",
    "$$\n",
    "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b},\n",
    "$$\n",
    "so if we wish to represent $E(\\mathbf{w})$ in this way, all we need to do is convert the sum operator to an inner product. We can get a vector from that sum operator by placing both $y_i$ and $f(\\mathbf{x}_i; \\mathbf{w})$ into vectors, which we do by defining \n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$\n",
    "and defining\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{X}; \\mathbf{w}) = \\begin{bmatrix}f(\\mathbf{x}_1; \\mathbf{w})\\\\f(\\mathbf{x}_2; \\mathbf{w})\\\\ \\vdots \\\\ f(\\mathbf{x}_n; \\mathbf{w})\\end{bmatrix}.\n",
    "$$\n",
    "The second of these is actually a vector-valued function. This term may appear intimidating, but the idea is straightforward. A vector valued function is simply a vector whose elements are themselves defined as *functions*, i.e. it is a vector of functions, rather than a vector of scalars. The idea is so straightforward, that we are going to ignore it for the moment, and barely use it in the derivation. But it will reappear later when we introduce *basis functions*. So we will, for the moment, ignore the dependence of $\\mathbf{f}$ on $\\mathbf{w}$ and $\\mathbf{X}$ and simply summarise it by a vector of numbers\n",
    "$$\n",
    "\\mathbf{f} = \\begin{bmatrix}f_1\\\\f_2\\\\ \\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$\n",
    "This allows us to write our objective in the folowing, linear algebraic form,\n",
    "$$\n",
    "E(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})\n",
    "$$\n",
    "from the rules of inner products.\n",
    "\n",
    "But what of our matrix $\\mathbf{X}$ of input data? At this point, we need to dust off [*matrix-vector multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication). Matrix multiplication is simply a convenient way of performing many inner products together, and it's exactly what we need to summarise the operation\n",
    "$$\n",
    "f_i = \\mathbf{x}_i^\\top\\mathbf{w}.\n",
    "$$\n",
    "This operation tells us that each element of the vector $\\mathbf{f}$ (our vector valued function) is given by an inner product between $\\mathbf{x}_i$ and $\\mathbf{w}$. In other words it is a series of inner products. Let's look at the definition of matrix multiplication, it takes the form\n",
    "$$\n",
    "\\mathbf{c} = \\mathbf{B}\\mathbf{a}\n",
    "$$\n",
    "where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can intepret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a $k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional vector ($k\\times 1$ dimensional matrix). \n",
    "\n",
    "The result of this multiplication is of the form\n",
    "$$\n",
    "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\ a_k\\end{bmatrix} = \n",
    "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
    "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\ \\vdots\\\\ c_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots + b_{1, k}a_k\\\\\n",
    "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\ \n",
    "\\vdots\\\\ \n",
    "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}\n",
    "$$\n",
    "so we see that each element of the result, $\\mathbf{a}$ is simply the inner product between each *row* of $\\mathbf{B}$ and the vector $\\mathbf{c}$. Because we have defined each element of $\\mathbf{f}$ to be given by the inner product between each *row* of the design matrix and the vector $\\mathbf{w}$ we now can write the full operation in one matrix multiplication,\n",
    "$$\n",
    "\\mathbf{f} = \\mathbf{X}\\mathbf{w}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.dot(X, w) # np.dot does matrix multiplication in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this result with our objective function,\n",
    "$$\n",
    "E(\\mathbf{w}) = (\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})\n",
    "$$\n",
    "we find we have defined the *model* with two equations. One equation tells us the form of our predictive function and how it depends on its parameters, the other tells us the form of our objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error function is: [[2.13078201]]\n"
     ]
    }
   ],
   "source": [
    "resid = (y-f)\n",
    "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
    "print(\"Error function is:\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Optimisation\n",
    "\n",
    "Our *model* has now been defined with two equations, the prediction function and the objective function. Next we will use multivariate calculus to define an *algorithm* to fit the model. The separation between model and algorithm is important and is often overlooked. Our model contains a function that shows how it will be used for prediction, and a function that describes the objective function we need to optimise to obtain a good set of parameters. \n",
    "\n",
    "The linear regression model we have described is still the same as the one we fitted above with a coordinate ascent algorithm. We have only played with the notation to obtain the same model in a matrix and vector notation. However, we will now fit this model with a different algorithm, one that is much faster. It is such a widely used algorithm that from the end user's perspective it doesn't even look like an algorithm, it just appears to be a single operation (or function). However, underneath the computer calls an algorithm to find the solution. Further, the algorithm we obtain is very widely used, and because of this it turns out to be highly optimised.\n",
    "\n",
    "Once again we are going to try and find the stationary points of our objective by finding the *stationary points*. However, the stationary points of a multivariate function, are a little bit more complext to find. Once again we need to find the point at which the derivative is zero, but now we need to use  *multivariate calculus* to find it. This involves learning a few additional rules of differentiation (that allow you to do the derivatives of a function with respect to  vector), but in the end it makes things quite a bit easier. We define vectorial derivatives as follows,\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}} = \\begin{bmatrix}\\frac{\\partial E(\\mathbf{w})}{\\partial w_1}\\\\\\frac{\\partial E(\\mathbf{w})}{\\partial w_2}\\end{bmatrix}.\n",
    "$$\n",
    "where $\\frac{\\partial E(\\mathbf{w})}{\\partial w_1}$ is the [partial derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the error function with respect to $w_1$.\n",
    "\n",
    "Differentiation through multiplications and additions is relatively straightforward, and since linear algebra is just multiplication and addition, then its rules of diffentiation are quite straightforward too, but slightly more complex than regular derivatives. \n",
    "\n",
    "### Matrix Differentiation\n",
    "\n",
    "We will need two rules of differentiation. The first is diffentiation of an inner product. By remebering that the inner product is made up of multiplication and addition, we can hope that its derivative is quite straightforward, and so it proves to be. We can start by thinking about the definition of the inner product,\n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i z_i,\n",
    "$$\n",
    "which if we were to take the derivative with respect to $z_k$ would simply return the gradient of the one term in the sum for which the derivative was non zero, that of $a_k$, so we know that \n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
    "$$\n",
    "and by our definition of multivariate derivatives we can simply stack all the partial derivatives of this form in a vector to obtain the result that\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
    "$$\n",
    "The second rule that's required is differentiation of a 'matrix quadratic'. A scalar quadratic in $z$ with coefficient $c$ has the form $cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a $k \\times k$ *matrix* of coefficients then the matrix quadratic form is written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a *scalar* quantity, but it is a function of a *vector*. \n",
    "\n",
    "#### Matching Dimensions in Matrix Multiplications\n",
    "\n",
    "There's a trick for telling that it's a scalar result. When you are doing maths with matrices, it's always worth pausing to perform a quick sanity check on the dimensions. Matrix multplication only works when the dimensions match. To be precise, the 'inner' dimension of the matrix must match. What is the inner dimension. If we multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the first of which has $k$ rows and $\\ell$ columns and the second of which has $p$ rows and $q$ columns, then we can check whether the multiplication works by writing the dimensionalities next to each other,\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times \\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
    "$$\n",
    "The inner dimensions are the two inside dimensions, $\\ell$ and $p$. The multiplication will only work if $\\ell=p$. The result of the multiplication will then be a $k\\times q$ matrix: this dimensionality comes from the 'outer dimensions'. Note that matrix multiplication is not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property). And if you change the order of the multiplication, \n",
    "$$\n",
    "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
    "$$\n",
    "firstly it may no longer even work, because now the condition is that $k=q$, and secondly the result could be of a different dimensionality. An exception is if the matrices are square matrices (e.g. same number of rows as columns) and they are both *symmetric*. A symmetric matrix is one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or equivalently, $a_{i,j} = a_{j,i}$ for all $i$ and $j$.  \n",
    "\n",
    "You will need to get used to working with matrices and vectors applying and developing new machine learning techniques. You should have come across them before, but you may not have used them as extensively as we will now do in this course. You should get used to using this trick to check your work and ensure you know what the dimension of an output matrix should be. For our matrix quadratic form, it turns out that we can see it as a special type of inner product.\n",
    "$$\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow \\mathbf{b}^\\top\\mathbf{z}\n",
    "$$\n",
    "where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is a scalar,\n",
    "$$\n",
    "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow (1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow (1\\times 1)\n",
    "$$\n",
    "where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
    "\n",
    "This implies that we should be able to differentiate this form, and indeed the rule for its differentiation is slightly more complex than the inner product, but still quite simple,\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top \\mathbf{z}.\n",
    "$$\n",
    "Note that in the special case where $\\mathbf{C}$ is symmetric then we have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to \n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= 2\\mathbf{C}\\mathbf{z}.\n",
    "$$\n",
    "### Differentiating the Objective\n",
    "\n",
    "First, we need to compute the full objective by substituting our prediction function into the objective function to obtain the objective in terms of $\\mathbf{w}$. Doing this we obtain\n",
    "$$\n",
    "E(\\mathbf{w})= (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w}).\n",
    "$$\n",
    "We now need to differentiate this *quadratic form* to find the minimum. We differentiate with respect to the *vector* $\\mathbf{w}$. But before we do that, we'll expand the brackets in the quadratic form to obtain a series of scalar terms. The rules for bracket expansion across the vectors are similar to those for the scalar system giving,\n",
    "$$\n",
    "(\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top \\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
    "$$\n",
    "which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{y}$ and $\\mathbf{b}=\\mathbf{d} = \\mathbf{X}\\mathbf{w}$ gives\n",
    "$$\n",
    "E(\\mathbf{w})= \\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{y}^\\top\\mathbf{X}\\mathbf{w} + \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "where we used the fact that $\\mathbf{y}^\\top\\mathbf{X}\\mathbf{w}= \\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{y}$. Now we can use our rules of differentiation to compute the derivative of this form, which is,\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{w}}E(\\mathbf{w})=- 2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w},\n",
    "$$\n",
    "where we have exploited the fact that $\\mathbf{X}^\\top\\mathbf{X}$ is symmetric to obtain this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Use the equivalence between our vector and our matrix formulations of linear regression, alongside our definition of vector derivates, to match the gradients we've computed directly for $\\frac{\\text{d}E(c, m)}{\\text{d}c}$ and $\\frac{\\text{d}E(c, m)}{\\text{d}m}$ to those for $\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Equation for Global Optimum\n",
    "\n",
    "Once again, we need to find the minimum of our objective function. Using our likelihood for multiple input regression we can now minimize for our parameter vector $\\mathbf{w}$. Firstly, just as in the single input case, we seek stationary points by finding parameter vectors that solve for when the gradients are zero,\n",
    "$$\n",
    "\\mathbf{0}=- 2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w},\n",
    "$$\n",
    "where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation we find the solution to be\n",
    "$$\n",
    "\\mathbf{w} = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$ \n",
    "where $\\mathbf{A}^{-1}$ denotes [*matrix inverse*](http://en.wikipedia.org/wiki/Invertible_matrix).\n",
    "\n",
    "### Solving the Multivariate System\n",
    "\n",
    "The solution for $\\mathbf{w}$ is given in terms of a matrix inverse, but computation of a matrix inverse requires, in itself, an algorithm to resolve it. You'll know this if you had to invert, by hand, a $3\\times 3$ matrix in high school. From a numerical stability perspective, it is also best not to compute the matrix inverse directly, but rather to ask the computer to *solve* the  system of linear equations given by\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "for $\\mathbf{w}$. This can be done in `numpy` using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can obtain the solution using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.88952457e+01]\n",
      " [-1.29806477e-02]]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map it back to the liner regression and plot the fit as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01298065]\n",
      "[28.89524568]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bcd3710>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = w[1]; c=w[0]\n",
    "f_test = m*x_test + c\n",
    "print(m)\n",
    "print(c)\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "A major advantage of the new system is that we can build a linear regression on a multivariate system. The matrix calculus didn't specify what the length of the vector $\\mathbf{x}$ should be, or equivalently the size of the design matrix. \n",
    "\n",
    "### Movie Body Count Data\n",
    "\n",
    "Let's load back in the movie body count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/film-death-counts-Python.csv',encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the features we've been provided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film, Year, Body_Count, MPAA_Rating, Genre, Director, Actors, Length_Minutes, IMDB_Rating\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(movies.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a design matrix based on the numeric features: year, Body_Count, Length_Minutes in an effort to predict the rating. We build the design matrix as follows:\n",
    "\n",
    "## Relation to Single Input System\n",
    "\n",
    "Bias as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features = ['Year', 'Body_Count', 'Length_Minutes']\n",
    "X = movies.loc[:, select_features]\n",
    "X['Eins'] = 1   # add a column for the offset\n",
    "y = movies[['IMDB_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a linear regression. But this time, we will create a pandas data frame for the result so we can store it in a form that we can visualise easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regression_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                regression_coefficient\n",
       "Year                         -0.016280\n",
       "Body_Count                   -0.000995\n",
       "Length_Minutes                0.025386\n",
       "Eins                         36.508363"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "w = pd.DataFrame(data=np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y)),  # solve linear regression here\n",
    "                 index = X.columns,  # columns of X become rows of w\n",
    "                 columns=['regression_coefficient']) # the column of X is the value of regression coefficient\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals to see how good our estimates are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x11bfdbb00>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEuNJREFUeJzt3X+UZ3V93/HnqxASZJQFN0xwIRna7DFyXGNkjjHa08yKtqhRSI80GIq7lp79Ryxtluoa09ieHI8Yo9b+Ss6eYsWEOFKCBdmmkRJHjzmFE9ZaF1wVagjuAkuMgIzS2DXv/vG9a8dlcGe+9zv7/c6H5+OcPfO9937u574/MPP63u/93h+pKiRJ7fob4y5AkrS2DHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9NJxkuTHkywmOWHctejpxaDXxEhyX5JXJNmepJK8/6jlF3XzP9xNz3TTi92/Q0luSfLKZfp9omvzSJI9Sc5eQT0fTvKdbr1vJLk1yU+tdjxHpqvq/qqaqqrvrrQPaRQMek2q/w38UpITl8x7I/CVZdpuqKop4KeBW4GPJ9l+VJvXdm3OBA4B/3aFdfxmt94m4CBwzcqHIE0Gg16T6iFgH/D3AJKcDrwUuPmpVqiqh6rqg8C/BN6T5Em/31X1f4AbgHNXU0xVPQFcD7zwyLwkfyvJHyf5yyRfT3Jdkg3dst8Ffhz4RPeJ4K1LPoGc2LVZSPIbSf4kyeNJPplk45L+35jkz7v+/8XRnxCklTLoNck+wmAvHuAS4Cbgr1aw3o3AGcBzj16Q5BnALwG3r6aQJKcAbwDuXTobeDfwHOB5wNkM3mSoqsuA++k+SVTVbz5F178MvKmr9yTgqm575wL/AbiUwaeQUxl8qpBWzaDXJPs4MJfkVAaB/5EVrvdA9/P0JfP+S5JHgW8CrwTeu8K+rurWexz428BlRxZU1b1VdWtV/VVV/QXwfuDnV9jvEf+pqr6yzCeG1wOfqKrPVtV3gF8HvDGVhmLQa2J14bcH+DVgY1X9yQpXPbLn+40l8y6qqg3ADwNXAJ9O8mMr6Ou3uvVmgCdY8ikhyRlJ5pMcTPJN4PeAjct385QeWvL628BU9/o5wNeOLKiqbwN/ucq+JcCg1+T7CLAT+N1VrPOLwMPAl49eUFXfraobge8y2ENfkaq6H7gS+GCSk7vZ72awl/2CqnoW8A8ZHM753mqrqPloDwJnHZnotvnsHv3pacyg16T7NINDLcc8SybJdJIrgHcCb6+qv16mTZJcCJwG7F9NIVV1K4PDQju6Wc8EFoFHk2wC/vlRqxwC/uZqtrHEDcBrk7w0yUnAv+L730SkFTPoNdFq4Laq+sYPaPZokm8xOEvn1cDFVfWho9p8Iskig2P07wK2VdXdQ5T0XuCtSX6YQfi+CHiMwSGmG49q+27g15I8muSq1Wykq+0twDyDvfvHGXxKWcmX0dL3iQ8ekSZfkingUWBzVf3ZuOvR+uIevTShkrw2yTO6Uzt/i8EnlvvGW5XWI4NeT2tJ7l5yC4Wl/y4dd23AhQy+E3gA2AxcUn4E1xA8dCNJjXOPXpIad+Kxm6y9jRs31szMTO9+vvWtb3HKKaf0L2jMHMdkcRyTp5Wx9B3H3r17v15VP3qsdhMR9DMzM9x55529+1lYWGBubq5/QWPmOCaL45g8rYyl7ziS/PlK2nnoRpIad8ygT/KhJA8nuWvJvNO7hzDc0/08rZufJP8myb1JvpDkRWtZvCTp2FayR/9h4IKj5u0CbquqzcBt3TTAqxicBraZwWXivz2aMiVJwzpm0FfVZ/j+uwDC4Pzea7vX1wIXLZn/ke6y9duBDUnOHFWxkqTVW9F59ElmgFuq6vnd9KPdrVuPLH+kqk5LcgtwdVV9tpt/G/C2qnrSN61JdtDdHGp6evq8+fn53oNZXFxkamrq2A0nnOOYLI5j8rQylr7j2Lp1696qmj1Wu1GfdbPc3fWWfSepqt3AboDZ2dkaxTfofhM/WRzHZGllHNDOWI7XOIY96+bQkUMy3c+Hu/kHGDxO7Yiz+P9P+5EkjcGwQX8zsK17vY3BszyPzH9jd/bNS4DHqurBnjVKkno45qGbJB8F5oCNSQ4weKjD1cD1SS5n8ADki7vm/5XB/cDvZfBYtDetQc2SpFU4ZtBX1RueYtH5y7Qt4M19i5IEM7v2jLS/nVsOs30Ffd539WtGul2Nn1fGSlLjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuIl4wpSkyTHq8/dXw3P414Z79JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcd6mWPoBZnbtYeeWw2wf4617pb7co5ekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrXK+iT/LMkdye5K8lHk/xIknOS3JHkniQfS3LSqIqVJK3e0EGfZBPwT4DZqno+cAJwCfAe4ANVtRl4BLh8FIVKkobT99DNicDJSU4EngE8CLwcuKFbfi1wUc9tSJJ6SFUNv3JyJfAu4Angk8CVwO1V9ZPd8rOBP+z2+I9edwewA2B6evq8+fn5oes4YnFxkampqd79jJvjmBz7Dj7G9Mlw6IlxV9LfehjHlk2nrqhdC79b0H8cW7du3VtVs8dqN/T96JOcBlwInAM8Cvxn4FXLNF32naSqdgO7AWZnZ2tubm7YUr5nYWGBUfQzbo5jcmzv7kf/vn3r/9EN62Ec9106t6J2LfxuwfEbR59DN68A/qyq/qKq/i9wI/BSYEN3KAfgLOCBnjVKknroE/T3Ay9J8owkAc4Hvgh8Cnh912YbcFO/EiVJfQwd9FV1B4MvXT8H7Ov62g28DfiVJPcCzwauGUGdkqQh9TpgV1XvBN551OyvAi/u068kaXS8MlaSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaN9mPhJf0tDKza8+K2u3ccpjtK2y7Evdd/ZqR9TWJ3KOXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcb2CPsmGJDck+VKS/Ul+LsnpSW5Nck/387RRFStJWr2+e/QfBP5bVf0U8NPAfmAXcFtVbQZu66YlSWMydNAneRbwd4BrAKrqO1X1KHAhcG3X7Frgor5FSpKGl6oabsXkhcBu4IsM9ub3AlcCB6tqw5J2j1TVkw7fJNkB7ACYnp4+b35+fqg6llpcXGRqaqp3P+PmOCbHvoOPMX0yHHpi3JX018o4YPRj2bLp1NF1tgp9/0a2bt26t6pmj9WuT9DPArcDL6uqO5J8EPgm8JaVBP1Ss7Ozdeeddw5Vx1ILCwvMzc317mfcHMfkmNm1h51bDvO+fev/8cqtjANGP5ZxPTO2799IkhUFfZ9j9AeAA1V1Rzd9A/Ai4FCSM7sizgQe7rENSVJPQwd9VT0EfC3Jc7tZ5zM4jHMzsK2btw24qVeFkqRe+n72eQtwXZKTgK8Cb2Lw5nF9ksuB+4GLe25DktRDr6Cvqs8Dyx0fOr9Pv5Kk0fHKWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalzvoE9yQpL/meSWbvqcJHckuSfJx5Kc1L9MSdKwRrFHfyWwf8n0e4APVNVm4BHg8hFsQ5I0pF5Bn+Qs4DXAf+ymA7wcuKFrci1wUZ9tSJL6SVUNv3JyA/Bu4JnAVcB24Paq+slu+dnAH1bV85dZdwewA2B6evq8+fn5oes4YnFxkampqd79jJvjmBz7Dj7G9Mlw6IlxV9JfK+OA0Y9ly6ZTR9fZKvT9G9m6deveqpo9VrsTh91Akl8AHq6qvUnmjsxepumy7yRVtRvYDTA7O1tzc3PLNVuVhYUFRtHPuDmOybF91x52bjnM+/YN/acyMVoZB4x+LPddOjeyvlbjeP2N9Pkv9TLgdUleDfwI8CzgXwMbkpxYVYeBs4AH+pcpSRrW0Mfoq+rtVXVWVc0AlwB/XFWXAp8CXt812wbc1LtKSdLQ1uI8+rcBv5LkXuDZwDVrsA1J0gqN5CBXVS0AC93rrwIvHkW/kqT+vDJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjRvIoQWmtzezaM+4SpHXLPXpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalx3utG0tPeuO6ltHPLYeaOw3bco5ekxhn0ktS4oYM+ydlJPpVkf5K7k1zZzT89ya1J7ul+nja6ciVJq9Vnj/4wsLOqnge8BHhzknOBXcBtVbUZuK2bliSNydBBX1UPVtXnutePA/uBTcCFwLVds2uBi/oWKUka3kiO0SeZAX4GuAOYrqoHYfBmAJwxim1IkoaTqurXQTIFfBp4V1XdmOTRqtqwZPkjVfWk4/RJdgA7AKanp8+bn5/vVQfA4uIiU1NTvfsZN8fxZPsOPjaSfoYxfTIcemJsmx+ZVsYB7Yxl+mQ44/RTh15/69ate6tq9ljtep1Hn+SHgD8ArquqG7vZh5KcWVUPJjkTeHi5datqN7AbYHZ2tubm5vqUAsDCwgKj6GfcHMeTbR/jM2N3bjnM+/at/0tOWhkHtDOWnVsO8w+Ow996n7NuAlwD7K+q9y9ZdDOwrXu9Dbhp+PIkSX31eUt8GXAZsC/J57t5vwpcDVyf5HLgfuDifiVKkvoYOuir6rNAnmLx+cP2K0kaLa+MlaTGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNW/+PaNFxNbOKJz3t3HJ4rE+GkjTgHr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOK2PXodVcnSpJ7tFLUuMMeklqnEEvSY0z6CWpcQa9JDXOs256WKuzX7yPu6RRco9ekhpn0EtS4wx6SWrcmgR9kguSfDnJvUl2rcU2JEkrM/IvY5OcAPx74JXAAeBPk9xcVV8c9bbg+78Q9UtMSXqytdijfzFwb1V9taq+A8wDF67BdiRJK5CqGm2HyeuBC6rqH3fTlwE/W1VXHNVuB7Cjm3wu8OURbH4j8PUR9DNujmOyOI7J08pY+o7jJ6rqR4/VaC3Oo88y8570blJVu4HdI91wcmdVzY6yz3FwHJPFcUyeVsZyvMaxFoduDgBnL5k+C3hgDbYjSVqBtQj6PwU2JzknyUnAJcDNa7AdSdIKjPzQTVUdTnIF8EfACcCHquruUW/nKYz0UNAYOY7J4jgmTytjOS7jGPmXsZKkyeKVsZLUOINekhrXZNAnuSpJJdk47lqGleQ3knwhyeeTfDLJc8Zd0zCSvDfJl7qxfDzJhnHXNIwkFye5O8lfJ1l3p/W1cFuSJB9K8nCSu8ZdSx9Jzk7yqST7u9+pK9d6m80FfZKzGdx+4f5x19LTe6vqBVX1QuAW4NfHXdCQbgWeX1UvAL4CvH3M9QzrLuDvA58ZdyGrteS2JK8CzgXekOTc8VY1lA8DF4y7iBE4DOysqucBLwHevNb/P5oLeuADwFtZ5iKt9aSqvrlk8hTW6Xiq6pNVdbibvJ3BdRXrTlXtr6pRXL09Dk3clqSqPgN8Y9x19FVVD1bV57rXjwP7gU1ruc2mnjCV5HXAwar6X8lyF+iuL0neBbwReAzYOuZyRuEfAR8bdxFPQ5uAry2ZPgD87Jhq0RJJZoCfAe5Yy+2su6BP8t+BH1tm0TuAXwX+7vGtaHg/aCxVdVNVvQN4R5K3A1cA7zyuBa7QscbRtXkHg4+s1x3P2lZjJeNYp1Z0WxIdX0mmgD8A/ulRn+BHbt0FfVW9Yrn5SbYA5wBH9ubPAj6X5MVV9dBxLHHFnmosy/h9YA8TGvTHGkeSbcAvAOfXBF+4sYr/H+uNtyWZMEl+iEHIX1dVN6719tZd0D+VqtoHnHFkOsl9wGxVrcs73CXZXFX3dJOvA740znqGleQC4G3Az1fVt8ddz9PU925LAhxkcFuSXx5vSU9fGeyJXgPsr6r3H49ttvhlbCuuTnJXki8wOBy15qdgrZF/BzwTuLU7VfR3xl3QMJL8YpIDwM8Be5L80bhrWqnuy/AjtyXZD1x/HG9LMjJJPgr8D+C5SQ4kuXzcNQ3pZcBlwMu7v4nPJ3n1Wm7QWyBIUuPco5ekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXH/D1SYbBaw/BaWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(y - np.dot(X, w)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows our model *hasn't* yet done a great job of representation, because the spread of values is large. We can check what the rating is dominated by in terms of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regression_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                regression_coefficient\n",
       "Year                         -0.016280\n",
       "Body_Count                   -0.000995\n",
       "Length_Minutes                0.025386\n",
       "Eins                         36.508363"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have to be a little careful about interpretation because our input values live on different scales, however it looks like we are dominated by the bias, with a small negative effect for later films (but bear in mind the years are large, so this effect is probably larger than it looks) and a positive effect for length. So it looks like long earlier films generally do better, but the residuals are so high that we probably haven't modelled the system very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution with QR Decomposition\n",
    "\n",
    "Performing a solve instead of a matrix inverse is the more numerically stable approach, but we can do even better. A [QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition) of a matrix factorises it into a matrix which is an orthogonal matrix $\\mathbf{Q}$, so that $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$. And a matrix which is upper triangular, $\\mathbf{R}$. \n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "(\\mathbf{Q}\\mathbf{R})^\\top (\\mathbf{Q}\\mathbf{R})\\boldsymbol{\\beta} = (\\mathbf{Q}\\mathbf{R})^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R}^\\top (\\mathbf{Q}^\\top \\mathbf{Q}) \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R}^\\top \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{Q}^\\top \\mathbf{y}\n",
    "$$\n",
    "This is a more numerically stable solution because it removes the need to compute $\\mathbf{X}^\\top\\mathbf{X}$ as an intermediate. Computing $\\mathbf{X}^\\top\\mathbf{X}$ is a bad idea because it involves squaring all the elements of $\\mathbf{X}$ and thereby potentially reducing the numerical precision with which we can represent the solution. Operating on $\\mathbf{X}$ directly preserves the numerical precision of the model.\n",
    "\n",
    "This can be more particularly seen when we begin to work with *basis functions* in the next week. Some systems that can be resolved with the QR decomposition can not be resolved by using solve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-0.016280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body_Count</th>\n",
       "      <td>-0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length_Minutes</th>\n",
       "      <td>0.025386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eins</th>\n",
       "      <td>36.508363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "Year            -0.016280\n",
       "Body_Count      -0.000995\n",
       "Length_Minutes   0.025386\n",
       "Eins            36.508363"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.linalg as linalg \n",
    "Q, R = np.linalg.qr(X)\n",
    "w = linalg.solve_triangular(R, np.dot(Q.T, y)) \n",
    "w = pd.DataFrame(w, index=X.columns)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code: it uses pods.notebook.display_prediction, but with a minor modification to \n",
    "# allow the use of ipywidgets\n",
    "from ipywidgets import *\n",
    "def display_prediction(basis, num_basis=4, wlim=(-1.,1.), fig=None, ax=None, xlim=None, ylim=None, num_points=1000, offset=0.0, **kwargs):\n",
    "    \"\"\"Interactive widget for displaying a prediction function based on summing separate basis functions.\n",
    "    :param basis: a function handle that calls the basis functions.\n",
    "    :type basis: function handle.\n",
    "    :param xlim: limits of the x axis to use.\n",
    "    :param ylim: limits of the y axis to use.\n",
    "    :param wlim: limits for the basis function weights.\"\"\"\n",
    "\n",
    "    #import numpy as np\n",
    "    #import pylab as plt\n",
    "\n",
    "    if fig is not None:\n",
    "        if ax is None:\n",
    "            ax = fig.gca()\n",
    "\n",
    "    if xlim is None:\n",
    "        if ax is not None:\n",
    "            xlim = ax.get_xlim()\n",
    "        else:\n",
    "            xlim = (-2., 2.)\n",
    "    if ylim is None:\n",
    "        if ax is not None:\n",
    "            ylim = ax.get_ylim()\n",
    "        else:\n",
    "            ylim = (-1., 1.)\n",
    "\n",
    "    # initialise X and set up W arguments.\n",
    "    x = np.zeros((num_points, 1))\n",
    "    x[:, 0] = np.linspace(xlim[0], xlim[1], num_points)\n",
    "    param_args = {}\n",
    "    for i in range(num_basis):\n",
    "        lim = list(wlim)\n",
    "        if i ==0:\n",
    "            lim[0] += offset\n",
    "            lim[1] += offset\n",
    "        param_args['w_' + str(i)] = tuple(lim)\n",
    "\n",
    "    # helper function for making basis prediction.\n",
    "    def predict_basis(w, basis, x, num_basis, **kwargs):\n",
    "        Phi = basis(x, num_basis, **kwargs)\n",
    "        f = np.dot(Phi, w)\n",
    "        return f, Phi\n",
    "    \n",
    "    if type(basis) is dict:\n",
    "        use_basis = basis[list(basis.keys())[0]]\n",
    "    else:\n",
    "        use_basis = basis\n",
    "    f, Phi = predict_basis(np.zeros((num_basis, 1)),\n",
    "                           use_basis, x, num_basis,\n",
    "                           **kwargs)\n",
    "    if fig is None:\n",
    "        fig, ax=plt.subplots(figsize=(12,4))\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "    predline = ax.plot(x, f, linewidth=2)[0]\n",
    "    basislines = []\n",
    "    for i in range(num_basis):\n",
    "        basislines.append(ax.plot(x, Phi[:, i], 'r')[0])\n",
    "\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    def generate_function(basis, num_basis, predline, basislines, basis_args, display_basis, offset, **kwargs):\n",
    "        w = np.zeros((num_basis, 1))\n",
    "        for i in range(num_basis):\n",
    "            w[i] = kwargs['w_'+ str(i)]\n",
    "        f, Phi = predict_basis(w, basis, x, num_basis, **basis_args)\n",
    "        predline.set_xdata(x[:, 0])\n",
    "        predline.set_ydata(f)\n",
    "        for i in range(num_basis):\n",
    "            basislines[i].set_xdata(x[:, 0])\n",
    "            basislines[i].set_ydata(Phi[:, i])\n",
    "\n",
    "        if display_basis:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(1) # make visible\n",
    "        else:\n",
    "            for i in range(num_basis):\n",
    "                basislines[i].set_alpha(0) \n",
    "        display(fig)\n",
    "    if type(basis) is not dict:\n",
    "        basis = fixed(basis)\n",
    "\n",
    "    plt.close(fig)\n",
    "    interact(generate_function, \n",
    "             basis=basis,\n",
    "             num_basis=fixed(num_basis),\n",
    "             predline=fixed(predline),\n",
    "             basislines=fixed(basislines),\n",
    "             basis_args=fixed(kwargs),\n",
    "             offset = fixed(offset),\n",
    "             display_basis = False,\n",
    "             **param_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "\n",
    "We've now seen how we may perform linear regression. Now, we are going to consider how we can perform *non-linear* regression. However, before we get into the details of how to do that we first need to consider in what ways the regression can be non-linear. \n",
    "\n",
    "Multivariate linear regression allows us to build models that take many features into account when making our prediction. In this session we are going to introduce *basis functions*. The term seems complicated, but they are actually based on rather a simple idea. If we are doing a multivariate linear regression, we get extra features that *might* help us predict our required response variable (or target value), $y$. But what if we only have one input value? We can actually artificially generate more input values with basis functions.\n",
    "\n",
    "### Non-linear in the Inputs\n",
    "\n",
    "When we refer to non-linear regression, we are normally referring to whether the regression is non-linear in the input space, or non-linear in the *covariates*. The covariates are the observations that move with the target (or *response*) variable. In our notation we have been using $\\mathbf{x}_i$ to represent a vector of the covariates associated with the $i$th observation. The coresponding response variable is $y_i$. If a model is non-linear in the inputs, it means that there is a non-linear function between the inputs and the response variable. Linear functions are functions that only involve multiplication and addition, in other words they can be represented through *linear algebra*. Linear regression involves assuming that a function takes the form\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}\n",
    "$$\n",
    "where $\\mathbf{w}$ are our regression weights. A very easy way to make the linear regression non-linear is to introduce non-linear functions. When we are introducing non-linear regression these functions are known as *basis functions*.\n",
    "\n",
    "### Basis Functions\n",
    "\n",
    "Here's the idea, instead of working directly on the original input space, $\\mathbf{x}$, we build models in a new space, $\\boldsymbol{\\phi}(\\mathbf{x})$ where $\\boldsymbol{\\phi}(\\cdot)$ is a *vector valued* function that is defined on the space $\\mathbf{x}$. \n",
    "\n",
    "Remember, that a vector valued function is just a vector that contains functions instead of values. Here's an example for a one dimensional input space, $x$, being projected to a *quadratic* basis. First we consider each basis function in turn, we can think of the elements of our vector as being indexed so that we have\n",
    "\\begin{align*}\n",
    "\\phi_1(x) = 1, \\\\\n",
    "\\phi_2(x) = x, \\\\\n",
    "\\phi_3(x) = x^2.\n",
    "\\end{align*}\n",
    "Now we can consider them together by placing them in a vector,\n",
    "$$\n",
    "\\boldsymbol{\\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
    "$$\n",
    "This is the idea of the vector valued function, we have simply collected the different functions together in the same vector making them notationally easier to deal with in our mathematics. \n",
    "\n",
    "When we consider the vector valued function for each data point, then we place all the data into a matrix. The result is a matrix valued function,\n",
    "$$\n",
    "\\boldsymbol{\\Phi}(\\mathbf{x}) = \n",
    "\\begin{bmatrix} 1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_n & x_n^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where we are still in the one dimensional input setting so $\\mathbf{x}$ here represents a vector of our inputs with $n$ elements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial basis extends the quadratic basis to arbitrary degree, so we might define the $j$th basis function associated with the model as\n",
    "$$\n",
    "\\phi_j(x_i) = x_i^j\n",
    "$$\n",
    "which can be implemented as a function in code as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = x**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in understanding how a basis works, we've provided you with a small interactive tool for exploring this polynomial basis. The tool can be summoned with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828666869a9d4071b274e885f4594cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='display_basis'), FloatSlider(value=0.0, description='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prediction(basis=polynomial, num_basis=4, ylim=[-3.,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try moving the sliders around to change the weight of each basis function. Click the control box `display_basis` to show the underlying basis functions (in red). The prediction function is shown in a thick blue line. *Warning* the sliders aren't presented quite in the correct order. `w_0` is associated with the bias, `w_1` is the linear term, `w_2` the quadratic and here (because we have four basis functions) we have `w_3` for the *cubic* term. So the subscript of the weight parameter is always associated with the corresponding polynomial's degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Try increasing the number of basis functions (thereby increasing the *degree* of the resulting polynomial). Describe what you see as you increase number of basis up to 10. Is it easy to change the function in intuitive ways? For example, if you want to manually set the weights $\\mathbf{w}$ for a particular training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to Data\n",
    "\n",
    "Now we are going to consider how these basis functions can be adjusted to fit to a particular data set. We return to the olympic marathon data. First we will scale the output of the data to be zero mean and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/maalvarezl/MLAI/master/Labs/datasets/olympic_marathon_men.csv', header=None, encoding= 'unicode_escape')\n",
    "x = np.array(data.iloc[:, 0].values).reshape(-1,1)\n",
    "y = np.array(data.iloc[:, 1].values).reshape(-1,1)\n",
    "y -= y.mean()\n",
    "y /= y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Now we are going to redefine our polynomial basis. Have a careful look at the operations we perform on `x` to create `z`. We use `z` in the polynomial computation. What are we doing to the inputs? Why do you think we are changing `x` in this manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s polynomial mlai.py\n",
    "def polynomial(x, num_basis=4, data_limits=[-1., 1.]):\n",
    "    \"Polynomial basis\"\n",
    "    centre = data_limits[0]/2. + data_limits[1]/2.\n",
    "    span = data_limits[1] - data_limits[0]\n",
    "    z = x - centre\n",
    "    z = 2*z/span\n",
    "    Phi = np.zeros((x.shape[0], num_basis))\n",
    "    for i in range(num_basis):\n",
    "        Phi[:, i:i+1] = z**i\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We like to make use of *design* matrices for our data. Design matrices, as you will recall, involve placing the data points into rows of the matrix and data features into the columns of the matrix. By convention, we are referencing a vector with a bold lower case letter, and a matrix with a bold upper case letter. The design matrix is therefore given by\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & \\mathbf{x} & \\mathbf{x}^2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Non-linear but linear in the Parameters\n",
    "\n",
    "One rather nice aspect of our model is that whilst it is non-linear in the inputs, it is still linear in the parameters $\\mathbf{w}$. This means that our derivations from before continue to operate to allow us to work with this model. In fact, although this is a non-linear regression it is still known as a *linear model* because it is linear in the parameters, \n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "where the vector $\\mathbf{x}$ appears inside the basis functions, making our result, $f(\\mathbf{x})$ non-linear in the inputs, but $\\mathbf{w}$ appears outside our basis function, making our result *linear* in the parameters. In practice, our basis function itself may contain its own set of parameters,\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}; \\boldsymbol{\\theta}),\n",
    "$$\n",
    "that we've denoted here as $\\boldsymbol{\\theta}$. If these parameters appear inside the basis function then our model is *non-linear* in these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "For the following prediction functions state whether the model is linear in the inputs, the parameters or both.\n",
    "\n",
    "(a) $f(x) = w_1x_1 + w_2$\n",
    "\n",
    "(b) $f(x) = w_1\\exp(x_1) + w_2x_2 + w_3$\n",
    "\n",
    "(c) $f(x) = \\log(x_1^{w_1}) + w_2x_2^2 + w_3$\n",
    "\n",
    "(d) $f(x) = \\exp(-\\sum_i(x_i - w_i)^2)$\n",
    "\n",
    "(e) $f(x) = \\exp(-\\mathbf{w}^\\top \\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model Yourself\n",
    "\n",
    "You now have everything you need to fit a non-linear (in the inputs) basis function model to the marathon data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "For this question, use the polynomial basis function. Compute the design matrix on the covariates (or input data), `x`. Use the design matrix and the response variable `y` to solve the following linear system for the model parameters `w`.\n",
    "$$\n",
    "\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\mathbf{w} = \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "Compute the corresponding error on the training data. How does it compare to the error you were able to achieve using the linear model with respect to the inputs? Plot the form of your prediction function from the least squares estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8 Answer Code\n",
    "# Write code for you answer to this question in this box"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
